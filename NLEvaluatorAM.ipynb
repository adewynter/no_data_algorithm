{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5b9f8b3e",
   "metadata": {},
   "source": [
    "# No-Data Algorithm \n",
    "\n",
    ">Applications to West Frisian\n",
    "\n",
    "Implementation of the second experiment of the paper, where we take a low-resource language and a standard rubric and show that this thing can detect lies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "54e47ad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shared functions/imports that you'll need\n",
    "import json\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "\n",
    "from llmclient import LLMClient, get_llm_response\n",
    "\n",
    "from no_data_utils import print_metrics\n",
    "from no_data_prompts import (get_evaluator_prompt_single_criteria,\n",
    "                             get_evaluator_prompt_all_criteria,\n",
    "                             ALL_CRIT_DEFAULT_RESPONSE)\n",
    "\n",
    "\n",
    "random.seed(123)\n",
    "rounds = 3 # EV Rounds\n",
    "\n",
    "def retrieve(prompt, llm, DEFAULT_RESPONSE={\"Label\": 0}, return_raw=False):\n",
    "    '''\n",
    "    Get the response from the LLM, accounting for failures and returning default responses as appropriate.\n",
    "    raw is for debugging only.\n",
    "    '''\n",
    "    response = None\n",
    "    tmp_response = None\n",
    "    tries = 0\n",
    "    while True:\n",
    "        if tries > 5: break\n",
    "        try:\n",
    "            response = get_llm_response(llm, prompt)\n",
    "            tmp_response = response\n",
    "            response = response.replace(\"```json\", \"\").replace(\"```\", \"\")\n",
    "            response = json.loads(response.strip(), strict=False)\n",
    "            if response is not None:\n",
    "                break\n",
    "        except:\n",
    "            response = None\n",
    "            tries += 1\n",
    "    if response is None:\n",
    "        if return_raw:\n",
    "            return DEFAULT_RESPONSE, tmp_response\n",
    "        return DEFAULT_RESPONSE, True\n",
    "    return response, False\n",
    "\n",
    "\n",
    "nl_data = json.load(open(\"west_frisian_dataset_with_rubric_shuffled.json\", \"r\", encoding=\"utf-8\")) #nl stands for \"natural-language\", not the language code for Dutch (nl-NL)\n",
    "other_exemplars = json.load(open(\"other_frisian_exemplars.json\", \"r\", encoding=\"utf-8\"))\n",
    "good_exemplars = json.load(open(\"good_frisian_exemplars.json\", \"r\", encoding=\"utf-8\"))\n",
    "\n",
    "# Ok so we use train for test and test to generate exemplars. \n",
    "# Code is too spaghetti to fix now\n",
    "train_data, test_data = nl_data[500:], nl_data[:500]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb47aa6f",
   "metadata": {},
   "source": [
    "## Baseline -- The Known Case\n",
    "\n",
    "We need to get the known cases. From here we'll characterise the model and note that this is the absolute maximum a model can get _if_ you knew the data.\n",
    "We will test multiple prompts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4044c86",
   "metadata": {},
   "source": [
    "### All criteria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1efff065",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "dev-gpt-41-longco-2025-04-14: 100%|██████████| 515/515 [2:17:10<00:00, 15.98s/it]\n",
      "dev-gpt-41-longco-2025-04-14: 100%|██████████| 515/515 [2:21:16<00:00, 16.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " --- IF ---\n",
      "dev-gpt-41-longco-2025-04-14\n",
      "--- no breakdown ---\n",
      "0.7631067961165049 0.8038585209003215\n",
      "--- breakdown no reasons ---\n",
      "0.5339805825242718 0.6816976127320955\n",
      "--- breakdown reasons ---\n",
      "0.46601941747572817 0.0\n",
      " --- OOF ---\n",
      "dev-gpt-41-longco-2025-04-14\n",
      "--- no breakdown ---\n",
      "0.5009708737864078 0.007722007722007722\n",
      "--- breakdown no reasons ---\n",
      "0.4854368932038835 0.00749063670411985\n",
      "--- breakdown reasons ---\n",
      "0.5145631067961165 0.6719160104986877\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "def baseline_all_criteria(models, use_other=False):\n",
    "    suff = \"other_rubric\" if use_other else \"good_rubric\"\n",
    "\n",
    "    for MODEL in models:\n",
    "        params = {\"max_tokens\": 128, \"temperature\": 0}\n",
    "        llm = LLMClient(params, MODEL)\n",
    "        labels = []\n",
    "        for ix in tqdm(range(len(train_data)), desc=MODEL):\n",
    "            entry = train_data[ix]\n",
    "            pred_entry = {k:v for k, v in entry.items()}\n",
    "\n",
    "            # No breakdown\n",
    "            llm.update_params(params)\n",
    "            prompt = get_evaluator_prompt_all_criteria(entry, request_breakdown=False, \n",
    "                                                       request_reasons=False, use_other=use_other)\n",
    "            response, _ = retrieve(prompt, llm)\n",
    "            labels.append(response[\"Label\"])\n",
    "            pred_entry[\"NoBreakdown\"] = response\n",
    "            # Breakdown no reasons\n",
    "            llm.update_params(params)\n",
    "            prompt = get_evaluator_prompt_all_criteria(entry, request_breakdown=True, \n",
    "                                                       request_reasons=False, use_other=use_other)\n",
    "            response, _ = retrieve(prompt, llm, DEFAULT_RESPONSE={\"c1\": 0, \"c2a\": 0, \"c2b\": 0, \"c3\": 0, \"c4\": 0, \"c5\": 0})\n",
    "            pred_entry[\"BreakdownNoReasons\"] = response\n",
    "\n",
    "            # Breakdown + reasons\n",
    "            prompt = get_evaluator_prompt_all_criteria(entry, request_breakdown=True, \n",
    "                                                       request_reasons=True, use_other=use_other)\n",
    "            response, _ = retrieve(prompt, llm, DEFAULT_RESPONSE=ALL_CRIT_DEFAULT_RESPONSE)\n",
    "            pred_entry[\"BreakdownReasons\"] = response\n",
    "\n",
    "            with open(f'{MODEL}_all_crit_{suff}.json', \"a\", encoding=\"utf-8\") as f:\n",
    "                f.write(json.dumps(pred_entry, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "\n",
    "def score_all_crit(models):\n",
    "    true_labels = []\n",
    "    for e in train_data:\n",
    "        true_labels.append(e[\"Label\"])\n",
    "\n",
    "    for model in models:\n",
    "        for rub in [(\"all_crit_good_rubric\", \"IF\"), (\"all_crit_other_rubric\", \"OOF\")]:\n",
    "            print(f\" --- {rub[-1]} ---\")\n",
    "            pred_data = [json.loads(l) for l in open(f\"{model}_{rub[0]}.json\", \"r\", encoding=\"utf-8\").readlines()]\n",
    "            breakdown_no_reasons, breakdown_reasons = {}, {}\n",
    "            no_breakdown = []\n",
    "            breakdown_no_reasons = []\n",
    "            breakdown_reasons = []\n",
    "\n",
    "            for e in pred_data:\n",
    "                no_breakdown.append(e[\"NoBreakdown\"][\"Label\"])\n",
    "                flip1, flip2 = False, False\n",
    "                for crit in [\"c1\", \"c2a\", \"c2b\", \"c3\", \"c4\", \"c5\"]:\n",
    "                    if e[\"BreakdownNoReasons\"][crit] == 0:\n",
    "                        flip1 = True\n",
    "                        break\n",
    "                    if e[\"BreakdownReasons\"][crit] == 0:\n",
    "                        flip2 = True\n",
    "                        break\n",
    "                breakdown_no_reasons.append(0 if flip1 else 1)\n",
    "                breakdown_reasons.append(0 if flip2 else 1)\n",
    "\n",
    "            print(model)\n",
    "            print(\"--- no breakdown ---\")\n",
    "            acc = accuracy_score(no_breakdown, true_labels)\n",
    "            f1 = f1_score(true_labels, no_breakdown)\n",
    "            print(acc, f1)\n",
    "\n",
    "            print(\"--- breakdown no reasons ---\")\n",
    "            acc = accuracy_score(breakdown_no_reasons, true_labels)\n",
    "            f1 = f1_score(true_labels, breakdown_no_reasons)\n",
    "            print(acc, f1)\n",
    "\n",
    "            print(\"--- breakdown reasons ---\")\n",
    "            acc = accuracy_score(breakdown_reasons, true_labels)\n",
    "            f1 = f1_score(true_labels, breakdown_reasons)\n",
    "            print(acc, f1)\n",
    "\n",
    "\n",
    "models = [\"gpt-41-longco-2025-04-14\"]\n",
    "baseline_all_criteria(models, use_other=False)\n",
    "baseline_all_criteria(models, use_other=True)\n",
    "\n",
    "score_all_crit(models)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e85c3e3f",
   "metadata": {},
   "source": [
    "### Per-criterion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02406d6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "dev-gpt-41-longco-2025-04-14: 100%|██████████| 515/515 [3:15:44<00:00, 22.81s/it]\n",
      "dev-gpt-41-longco-2025-04-14: 100%|██████████| 515/515 [2:48:29<00:00, 19.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ==== IF ===\n",
      "------- dev-gpt-41-longco-2025-04-14 ---------\n",
      "--- no reasons ---\n",
      " --- c1: 0.97, 0.99\n",
      " --- c2a: 0.81, 0.89\n",
      " --- c2b: 0.86, 0.91\n",
      " --- c3: 0.93, 0.96\n",
      " --- c4: 0.95, 0.97\n",
      " --- c5: 0.85, 0.92\n",
      "--- reasons ---\n",
      "0.8 0.81\n",
      " --- c1: 0.95, 0.97\n",
      " --- c2a: 0.75, 0.86\n",
      " --- c2b: 0.85, 0.9\n",
      " --- c3: 0.92, 0.96\n",
      " --- c4: 0.95, 0.97\n",
      " --- c5: 0.8, 0.89\n",
      " ==== OOF ===\n",
      "------- dev-gpt-41-longco-2025-04-14 ---------\n",
      "--- no reasons ---\n",
      " --- c1: 0.03, 0.02\n",
      " --- c2a: 0.68, 0.81\n",
      " --- c2b: 0.2, 0.02\n",
      " --- c3: 0.83, 0.91\n",
      " --- c4: 0.87, 0.91\n",
      " --- c5: 0.11, 0.12\n",
      "--- reasons ---\n",
      "0.5 0.0\n",
      " --- c1: 0.03, 0.02\n",
      " --- c2a: 0.67, 0.8\n",
      " --- c2b: 0.19, 0.01\n",
      " --- c3: 0.84, 0.91\n",
      " --- c4: 0.71, 0.78\n",
      " --- c5: 0.15, 0.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def baseline_per_criteria(models, use_other=False):\n",
    "    suff = \"other_rubric\" if use_other else \"good_rubric\"\n",
    "\n",
    "    for MODEL in models:\n",
    "        params = {\"max_tokens\": 128, \"temperature\": 0}\n",
    "        llm = LLMClient(params, MODEL)\n",
    "        labels, labels_reasons, true_labels = {}, {}, {}\n",
    "        for ix in tqdm(range(len(train_data)), desc=MODEL):\n",
    "            entry = train_data[ix]\n",
    "            pred_entry = {k:v for k, v in entry.items()}\n",
    "\n",
    "            pred_entry[\"NoReasons\"] = {}\n",
    "            for crit in [\"c1\", \"c2a\", \"c2b\", \"c3\", \"c4\", \"c5\"]:\n",
    "                prompt = get_evaluator_prompt_single_criteria(entry, \n",
    "                                                              crit, \n",
    "                                                              request_reasons=False,\n",
    "                                                              use_other=use_other)\n",
    "                response, _ = retrieve(prompt, llm, DEFAULT_RESPONSE={crit: 0})\n",
    "                pred_entry[\"NoReasons\"][crit] = response[crit]\n",
    "\n",
    "                if crit not in labels: labels[crit] = []\n",
    "                labels[crit].append(pred_entry[\"NoReasons\"][crit])\n",
    "                if crit not in true_labels: true_labels[crit] = []\n",
    "                true_labels[crit].append(entry[\"Rubric\"][crit])\n",
    "\n",
    "            pred_entry[\"Reasons\"] = {}\n",
    "            for crit in [\"c1\", \"c2a\", \"c2b\", \"c3\", \"c4\", \"c5\"]:\n",
    "                # Breakdown no reasons\n",
    "                prompt = get_evaluator_prompt_single_criteria(entry, \n",
    "                                                              crit, request_reasons=True,\n",
    "                                                              use_other=use_other)\n",
    "                response, _ = retrieve(prompt, llm, DEFAULT_RESPONSE={crit: 0, crit + \"_reason\": \"FAIL\"})\n",
    "                pred_entry[\"Reasons\"][crit] = response[crit]\n",
    "\n",
    "                if crit not in labels_reasons: labels_reasons[crit] = []\n",
    "                labels_reasons[crit].append(pred_entry[\"Reasons\"][crit])\n",
    "\n",
    "            with open(f'{MODEL}_per_crit_{suff}.json', \"a\", encoding=\"utf-8\") as f:\n",
    "                f.write(json.dumps(pred_entry, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "\n",
    "def score_per_criteria(models):\n",
    "\n",
    "    true_labels_per_crit = {k: [] for k in [\"c1\", \"c2a\", \"c2b\", \"c3\", \"c4\", \"c5\"]}\n",
    "    true_labels = []\n",
    "    for e in train_data:\n",
    "        true_labels.append(e[\"Label\"])\n",
    "        for crit in [\"c1\", \"c2a\", \"c2b\", \"c3\", \"c4\", \"c5\"]:\n",
    "            true_labels_per_crit[crit].append(e[\"Rubric\"][crit])\n",
    "\n",
    "    for model in models:\n",
    "        for rub in [(\"per_crit_good_rubric\", \"IF\"), (\"per_crit_other_rubric\", \"OOF\")]:\n",
    "            print(f\" ==== {rub[-1]} ===\")\n",
    "            pred_data = [json.loads(l) for l in open(f\"{model}_{rub[0]}.json\", \"r\", encoding=\"utf-8\").readlines()]\n",
    "\n",
    "            print(f\"------- {model} ---------\")\n",
    "            no_reasons, reasons = [], []\n",
    "            preds_per_crit_reasons = {k: [] for k in [\"c1\", \"c2a\", \"c2b\", \"c3\", \"c4\", \"c5\"]}\n",
    "            preds_per_crit_no_reasons = {k: [] for k in [\"c1\", \"c2a\", \"c2b\", \"c3\", \"c4\", \"c5\"]}\n",
    "            for e in pred_data:\n",
    "                # Label prediction\n",
    "                flip1, flip2 = False, False\n",
    "                for crit in [\"c1\", \"c2a\", \"c2b\", \"c3\", \"c4\", \"c5\"]:\n",
    "                    preds_per_crit_reasons[crit].append(e[\"Reasons\"][crit])\n",
    "                    preds_per_crit_no_reasons[crit].append(e[\"NoReasons\"][crit])\n",
    "                    if e[\"Reasons\"][crit] == 0:\n",
    "                        flip1 = True\n",
    "                    if e[\"NoReasons\"][crit] == 0:\n",
    "                        flip2 = True\n",
    "                reasons.append(0 if flip1 else 1)\n",
    "                no_reasons.append(0 if flip2 else 1)\n",
    "\n",
    "            print(\"--- no reasons ---\")\n",
    "            acc = round(accuracy_score(no_reasons, true_labels), 2)\n",
    "            f1 =  round(f1_score(no_reasons, true_labels), 2)\n",
    "            for k, v in preds_per_crit_no_reasons.items():\n",
    "                c_acc = round(accuracy_score(preds_per_crit_no_reasons[k], \n",
    "                                            true_labels_per_crit[k]), 2)\n",
    "                c_f1 = round(f1_score(preds_per_crit_no_reasons[k], \n",
    "                                    true_labels_per_crit[k]), 2)\n",
    "                print(f\" --- {k}: {c_acc}, {c_f1}\")\n",
    "\n",
    "            print(\"--- reasons ---\")\n",
    "            acc = round(accuracy_score(reasons, true_labels), 2)\n",
    "            f1 = round(f1_score(reasons, true_labels), 2)\n",
    "            print(acc, f1)\n",
    "            for k, v in preds_per_crit_reasons.items():\n",
    "                c_acc = round(accuracy_score(preds_per_crit_reasons[k], \n",
    "                                            true_labels_per_crit[k]), 2)\n",
    "                c_f1 = round(f1_score(preds_per_crit_reasons[k], \n",
    "                                    true_labels_per_crit[k]), 2)\n",
    "                print(f\" --- {k}: {c_acc}, {c_f1}\")\n",
    "\n",
    "\n",
    "models = [\"dev-gpt-41-longco-2025-04-14\"]\n",
    "baseline_per_criteria(models, use_other=False)\n",
    "baseline_per_criteria(models, use_other=True)\n",
    "score_per_criteria(models)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c29d35bb",
   "metadata": {},
   "source": [
    "# No-Data Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0f88d2c",
   "metadata": {},
   "source": [
    "### Shared stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "854ae293",
   "metadata": {},
   "outputs": [],
   "source": [
    "from no_data_prompts import (get_generator_prompt,\n",
    "                             get_evaluator_prompt_all_criteria,\n",
    "                             get_evaluator_prompt_single_criteria,\n",
    "                             good_rubric_nl,\n",
    "                             other_rubric_nl,\n",
    "                             get_generator_prompt_with_picker)\n",
    "\n",
    "\n",
    "def no_data_algorithm_with_flips(X, evaluator, generator, max_rounds=3, phi=0.3, \n",
    "                                 use_other=False):\n",
    "    '''\n",
    "    The actual implementation of the No-Data algorithm, flipping labels.\n",
    "    '''\n",
    "    labels = []\n",
    "    flips = []\n",
    "    successes = []\n",
    "    for i in tqdm(range(len(X))):\n",
    "        x = X[i]\n",
    "        success, label, reason = ev_protocol(x, evaluator, generator, max_rounds, \n",
    "                                             use_other=use_other)\n",
    "        if success:\n",
    "            flips.append([False, reason] if \"Random\" not in reason else [True, reason])\n",
    "            labels.append(label)\n",
    "            successes.append(1)\n",
    "        else:\n",
    "            successes.append(0)\n",
    "            if random.uniform(0, 1) < phi:\n",
    "                labels.append(label)\n",
    "                flips.append([False, \"Random flip\"])\n",
    "            else:\n",
    "                label = not label\n",
    "                labels.append(label)\n",
    "                flips.append([True, \"Random flip\"])\n",
    "\n",
    "    return labels, successes, flips\n",
    "\n",
    "\n",
    "def ev_protocol(x, evaluator, generator, max_rounds, use_other=False):\n",
    "    ''' \n",
    "    Quick and dirty implementation of the EV Protocol. In here, \n",
    "    we understand lying as generating a datapoint that is not within\n",
    "    the correct phenomenon. \n",
    "    '''\n",
    "    # Overwrite the criteria in the criterion-free style\n",
    "    x = label_criteria_for(x, use_other=False)\n",
    "\n",
    "    for _ in range(max_rounds):\n",
    "        # Step 1: generate x \\cong x' based on the belief of y\n",
    "        y_tilde = evaluator(x, call_verifier=False, use_other=use_other)\n",
    "        # Always generate with the rubric you know\n",
    "        x_tilde = generator(x, y_tilde, use_other=use_other)\n",
    "        # Step 2: get the challenge\n",
    "        check = random.choice([check1, check2])\n",
    "        # Step 3: git checkd\n",
    "        success, reason = check(x, x_tilde, y_tilde=y_tilde, use_other=use_other, \n",
    "                                evaluator=evaluator)\n",
    "        # Last bugfix: if the label is zero, the checks should fail... right?\n",
    "        if not success: break\n",
    "    return success, y_tilde, (reason, x, x_tilde)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fecb95ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_criteria_for(x, use_other=False):\n",
    "    crits = {}\n",
    "    for i, c in enumerate([\"c1\", \"c2a\", \"c2b\", \"c3\", \"c4\", \"c5\"]):\n",
    "        prompt = get_evaluator_prompt_single_criteria(entry=x, \n",
    "                                                      criterion=c, \n",
    "                                                      request_reasons=True, \n",
    "                                                      use_other=use_other)\n",
    "        label_x, _ = retrieve(prompt, llm_verifier,  \n",
    "                              DEFAULT_RESPONSE={c: 0, f\"{c}_reason\": \"FAIL\"})\n",
    "        crits[c] = label_x[c]\n",
    "    x[\"Rubric\"] = crits\n",
    "    return x\n",
    "\n",
    "\n",
    "def check1(x, x_tilde, y_tilde=None, evaluator=None, round=None, use_other=False):\n",
    "    ''' \n",
    "    Assert that a subset matching the criteria exists in both\n",
    "    x_tilde and x.\n",
    "    I.e., verify the isomorphism for IF. \n",
    "    This one is trickier to implement because we need to know in advance whether \n",
    "    the rubric is decomposable.\n",
    "    '''\n",
    "    faileds = 0\n",
    "    for i, c in enumerate([\"c1\", \"c2a\", \"c2b\", \"c3\", \"c4\", \"c5\"]):\n",
    "        if use_other and c == \"c3\":\n",
    "            if int(x_tilde[\"Rubric\"][c]) != int(x[\"Rubric\"][\"c2a\"]^x[\"Rubric\"][\"c2b\"]):\n",
    "                faileds += 1\n",
    "        else:\n",
    "            if int(x[\"Rubric\"][c]) != int(x_tilde[\"Rubric\"][c]):\n",
    "                faileds += 1\n",
    "\n",
    "    if faileds > 0:\n",
    "        return False, f\"Ch1 Failed {faileds} times\"\n",
    "    return True, \"\"\n",
    "\n",
    "\n",
    "def check2(x, x_tilde, y_tilde, evaluator=None, round=None, use_other=False):\n",
    "    ''' \n",
    "    Assert that the output of the criteria matches in both cases.\n",
    "    Take in an estimated and given datapoints x_tilde, x, and a rubric.\n",
    "    Since we do not know the label, we exact match (up to MAX_C2_ALLOWABLE_FAILS) the criterion.\n",
    "    Return True/False and the reason.\n",
    "    '''\n",
    "    def get_encoding(c):\n",
    "        enc = \"\".join([str(v) for k, v in c.items() if \"reason\" not in k])\n",
    "        return enc\n",
    "\n",
    "    faileds = 0    \n",
    "    tilde_encoding = get_encoding(x_tilde[\"Rubric\"])\n",
    "\n",
    "    for i, c in enumerate([\"c1\", \"c2a\", \"c2b\", \"c3\", \"c4\", \"c5\"]):\n",
    "        label_x = x[\"Rubric\"]\n",
    "        if int(label_x[c]) != int(tilde_encoding[i]):\n",
    "            faileds += 1\n",
    "\n",
    "    if faileds > 0:\n",
    "        return False, f\"Ch1 Failed {faileds} times\"\n",
    "    return True, \"\"\n",
    "\n",
    "\n",
    "def get_score_from_crits(r):\n",
    "    label = 1\n",
    "    for crit in [\"c1\", \"c2a\", \"c2b\", \"c3\", \"c4\", \"c5\"]:\n",
    "        if r[crit] == 0:\n",
    "            label = 0\n",
    "            break\n",
    "    return label\n",
    "\n",
    "\n",
    "def llm_evaluator_fn(x, call_verifier=False, use_other=False):\n",
    "    \"\"\" Take in a datapoint, return an estimated label y_tilde.\n",
    "    General purpose function to get a label using the (unknown) aggregator function.\n",
    "    Note that the LLM 'knows' this aggregator.\n",
    "    This is the EVALUATOR estimating the label, NOT the verifier\n",
    "    \"\"\"\n",
    "    \n",
    "    if not call_verifier:\n",
    "        prompt = get_evaluator_prompt_all_criteria(x,\n",
    "                                                   request_breakdown=True,\n",
    "                                                   request_reasons=True, \n",
    "                                                   use_other=use_other)\n",
    "        response, _ = retrieve(prompt, llm_evaluator, DEFAULT_RESPONSE=ALL_CRIT_DEFAULT_RESPONSE)\n",
    "    else:\n",
    "        prompt = get_evaluator_prompt_all_criteria(x,\n",
    "                                                   request_breakdown=True,\n",
    "                                                   request_reasons=True, \n",
    "                                                   use_other=False) # Never true for the verifier\n",
    "        response, _ = retrieve(prompt, llm_verifier, DEFAULT_RESPONSE=ALL_CRIT_DEFAULT_RESPONSE)\n",
    "\n",
    "    label_x = get_score_from_crits(response)\n",
    "    return label_x\n",
    "\n",
    "\n",
    "def llm_generator_fn(x, y_tilde, use_other=False):\n",
    "    \"\"\" Take in a datapoint, estimated label y_tilde, and a rubric in the form of \"use other\".\n",
    "    Return an x_tilde.\n",
    "    \"\"\"\n",
    "    default_response = {\n",
    "        \"Prompt\": \"\",\n",
    "        \"Output\": \"\"\n",
    "    }\n",
    "    # First get the criteria, according to itself\n",
    "    crits = {}\n",
    "    for i, c in enumerate([\"c1\", \"c2a\", \"c2b\", \"c3\", \"c4\", \"c5\"]):\n",
    "        prompt = get_evaluator_prompt_single_criteria(entry=x, \n",
    "                                                      criterion=c, \n",
    "                                                      request_reasons=True, \n",
    "                                                      use_other=use_other) \n",
    "        label_x, fail_state = retrieve(prompt, llm_evaluator, DEFAULT_RESPONSE={c: 0, f\"{c}_reason\": \"FAIL\"})\n",
    "        if fail_state: print(f\"evaluator fail {c}\")\n",
    "        crits[c] = label_x[c]\n",
    "        crits[c + \"_reason\"] = label_x[c + \"_reason\"]\n",
    "\n",
    "    prompt = get_generator_prompt(x, y_tilde,\n",
    "                                  criteria=crits, \n",
    "                                  use_other=use_other)\n",
    "    response, fail_state = retrieve(prompt, llm_generator, DEFAULT_RESPONSE=default_response)\n",
    "    if \"Output\" not in response or \"Prompt\" not in response:\n",
    "        response = default_response\n",
    "    response[\"Rubric\"] = crits\n",
    "    response[\"Label\"] = y_tilde\n",
    "    return response\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "188096fc",
   "metadata": {},
   "source": [
    "### Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fad7296",
   "metadata": {},
   "outputs": [],
   "source": [
    "verifier_model = \"gpt-41-longco-2025-04-14\"\n",
    "evaluator_model = \"gpt-41-longco-2025-04-14\"\n",
    "\n",
    "params = {\"max_tokens\": 256, \"temperature\": 0}\n",
    "llm_evaluator = LLMClient(params, evaluator_model)\n",
    "params = {\"max_tokens\": 5000, \"temperature\": 0}\n",
    "llm_generator = LLMClient(params, evaluator_model)\n",
    "params = {\"max_tokens\": 256, \"temperature\": 0}\n",
    "llm_verifier = LLMClient(params, verifier_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "70c2387f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 515/515 [12:15:54<00:00, 85.74s/it]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In-phenomenon test score: 0.744 | Successes: 86.796 | F1: 78.073 | Flips: 4.854\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 515/515 [9:29:35<00:00, 66.36s/it]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out-phenomenon test score: 0.491 | Successes: 1.165 | F1: 39.07 | Flips: 33.592\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "phi = 0.7 \n",
    "\n",
    "good_flips, other_flips = 0, 0\n",
    "test_set = [(None, p[\"Label\"]) for p in train_data]\n",
    "good_flips, other_flips = None, None\n",
    "\n",
    "good_labels, good_successes, good_flips = no_data_algorithm_with_flips(train_data, \n",
    "                                                evaluator=llm_evaluator_fn,\n",
    "                                                generator=llm_generator_fn,\n",
    "                                                phi=phi,\n",
    "                                                max_rounds=rounds,\n",
    "                                                use_other=False)\n",
    "print_metrics(good_labels, good_successes, test_set, \"In\", good_flips)\n",
    "\n",
    "other_labels, other_successes, other_flips = no_data_algorithm_with_flips(train_data, \n",
    "                                                  evaluator=llm_evaluator_fn,\n",
    "                                                  generator=llm_generator_fn,\n",
    "                                                  phi=phi,\n",
    "                                                  max_rounds=rounds,\n",
    "                                                  use_other=True)\n",
    "\n",
    "print_metrics(other_labels, other_successes, test_set, \"Out\", other_flips)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b2a779c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 515/515 [12:57:41<00:00, 90.61s/it]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In-phenomenon test score: 0.724 | Successes: 87.767 | F1: 76.568 | Flips: 9.32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 515/515 [9:34:32<00:00, 66.94s/it]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out-phenomenon test score: 0.493 | Successes: 1.359 | F1: 57.835 | Flips: 70.291\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "phi = 0.3\n",
    "\n",
    "good_flips, other_flips = 0, 0\n",
    "test_set = [(None, p[\"Label\"]) for p in train_data]\n",
    "good_flips, other_flips = None, None\n",
    "\n",
    "good_labels, good_successes, good_flips = no_data_algorithm_with_flips(train_data, \n",
    "                                                evaluator=llm_evaluator_fn,\n",
    "                                                generator=llm_generator_fn,\n",
    "                                                phi=phi, \n",
    "                                                use_other=False,\n",
    "                                                max_rounds=rounds)\n",
    "print_metrics(good_labels, good_successes, test_set, \"In\", good_flips)\n",
    "\n",
    "other_labels, other_successes, other_flips = no_data_algorithm_with_flips(train_data, \n",
    "                                                                        evaluator=llm_evaluator_fn,\n",
    "                                                                        generator=llm_generator_fn,\n",
    "                                                                        phi=phi,\n",
    "                                                                        max_rounds=rounds,\n",
    "                                                                        use_other=True)\n",
    "\n",
    "print_metrics(other_labels, other_successes, test_set, \"Out\", other_flips)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c942297",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 515/515 [12:08:51<00:00, 84.92s/it]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In-phenomenon test score: 0.769 | Successes: 86.214 | F1: 79.796 | Flips: 1.748\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 515/515 [9:28:20<00:00, 66.22s/it]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out-phenomenon test score: 0.507 | Successes: 1.165 | F1: 16.447 | Flips: 9.126\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "phi = 0.9\n",
    "\n",
    "good_flips, other_flips = 0, 0\n",
    "test_set = [(None, p[\"Label\"]) for p in train_data]\n",
    "good_flips, other_flips = None, None\n",
    "\n",
    "good_labels, good_successes, good_flips = no_data_algorithm_with_flips(train_data, \n",
    "                                                evaluator=llm_evaluator_fn,\n",
    "                                                generator=llm_generator_fn,\n",
    "                                                phi=phi, \n",
    "                                                use_other=False,\n",
    "                                                max_rounds=rounds)\n",
    "\n",
    "print_metrics(good_labels, good_successes, test_set, \"In\", good_flips)\n",
    "\n",
    "other_labels, other_successes, other_flips = no_data_algorithm_with_flips(train_data, \n",
    "                                                  evaluator=llm_evaluator_fn,\n",
    "                                                  generator=llm_generator_fn,\n",
    "                                                  phi=phi,\n",
    "                                                  use_other=True,\n",
    "                                                  max_rounds=rounds)\n",
    "\n",
    "print_metrics(other_labels, other_successes, test_set, \"Out\", other_flips)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
