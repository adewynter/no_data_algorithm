{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c1db33c0",
   "metadata": {},
   "source": [
    "# The No-Data Algorithm\n",
    "\n",
    "This is an implementation of two toy examples (Experiment 1 in the paper) to illustrate the inner workings of the No-Data Algorithm. \n",
    "\n",
    "The idea is to generate a 'good' (in-phenomenon) dataset with a rubric, and a bad ('other'; out-of-phenomenon) dataset with a separate rubric. \n",
    "We do not call these in-distribution / out-of-distribution for learning-theoretical reasons. \n",
    "\n",
    "The evaluator is a decision tree trained with the 'good' data, for which we know the accuracy in 'good' (and that it cannot possibly know 'other'). The rationale is that the decision tree _learnt_ the rubric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "263f7389",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shared functions/imports that you'll need\n",
    "import json\n",
    "import random\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "import math\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "K = 12 # Bitstring length\n",
    "rounds = 3 # EV Rounds\n",
    "\n",
    "def print_metrics(labels, successes, test_set, suff, flips=None, return_instead=False):\n",
    "    Y = [int(p[-1]) for p in test_set]\n",
    "    _good_labels = [int(g) for g in labels]\n",
    "    acc = round(accuracy_score(Y, _good_labels), 3)\n",
    "    f1 = round(f1_score(Y, _good_labels)*100., 3)\n",
    "    succ = round(sum(successes)*100/len(test_set), 3)\n",
    "    if flips is not None:\n",
    "        flips = [int(g[0]) for g in flips]\n",
    "        flips = round(sum(flips)*100/len(test_set), 3)\n",
    "        if not return_instead:\n",
    "            print(f\"{suff}-phenomenon test score: {acc} | Successes: {succ} | F1: {f1} | Flips: {flips}\")\n",
    "        else:\n",
    "            return acc, f1, succ, flips\n",
    "    else:\n",
    "        if not return_instead:\n",
    "            print(f\"{suff}-phenomenon test score: {acc} | Successes: {succ} | F1: {f1}\")\n",
    "        else:\n",
    "            return acc, f1, succ\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86434cfe",
   "metadata": {},
   "source": [
    "## Data Generation, Rubrics, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7580e1d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataset_generator(n, k, rubric, balance=False, is_prime=False):\n",
    "    '''\n",
    "    Generate a dataset of k-ary binary strings given a rubric. \n",
    "    '''\n",
    "    dataset = []\n",
    "    ones, zeros = 0, 0\n",
    "    while len(dataset) < n:\n",
    "        x = \"\".join([random.choice([\"0\", \"1\"]) for _ in range(k)])\n",
    "        answer = apply_rubric_to(x, rubric)\n",
    "        y = get_label_for(answer, is_prime=is_prime)\n",
    "        if balance:\n",
    "            if y == \"1\" and ones > n // 2:\n",
    "                continue\n",
    "            if y == \"0\" and zeros > n // 2:\n",
    "                continue\n",
    "        dataset.append((x, y))\n",
    "        if y == \"1\":\n",
    "            ones += 1\n",
    "        else:\n",
    "            zeros += 1\n",
    "\n",
    "    random.shuffle(dataset)\n",
    "    if balance:\n",
    "        one_labels = [p for p in dataset if p[-1] == \"1\"]\n",
    "        zero_labels = [p for p in dataset if p[-1] == \"0\"]\n",
    "        dataset = []\n",
    "        for a, b in zip(one_labels, zero_labels):\n",
    "            dataset.append(a)\n",
    "            dataset.append(b)        \n",
    "    return dataset\n",
    "\n",
    "\n",
    "def apply_rubric_to(x, rubric):\n",
    "    '''\n",
    "    Apply a given rubric (set of criteria) to the given input.\n",
    "    '''\n",
    "    criteria_output = []\n",
    "    for criteria in rubric:\n",
    "        label = criteria(x)\n",
    "        criteria_output.append(str(label))\n",
    "    return \"\".join(criteria_output)\n",
    "\n",
    "\n",
    "def get_label_for(x, is_prime=False):\n",
    "    '''\n",
    "    Simple majority vote aggregator, with tiebreaking.\n",
    "    When it is prime, it is a mixture of tiebreaking and some hardcoded\n",
    "    rules.\n",
    "    '''\n",
    "    _z = Counter(x)\n",
    "    z = _z.most_common()\n",
    "    vote = z[0][0]\n",
    "    if len(z) > 1:\n",
    "        # This will not be called for a 3-criteria rubric\n",
    "        # but will for the 6-criteria ones. We default to 1\n",
    "        # in case of ties\n",
    "        if z[0][-1] == z[1][-1]:\n",
    "            vote = \"1\"\n",
    "\n",
    "    return vote\n",
    "\n",
    "\n",
    "def what_matched_what(x, criterion, position=None, is_full_string=False):\n",
    "    ''' \n",
    "    Helper function to determine subsets that matched a given criterion.\n",
    "    We only need ONE subset fulfiling the criterion. \n",
    "    The difficulty lies on distinguishing criteria that evaluate\n",
    "    the whole string, positional values, versus any subset. \n",
    "    \n",
    "    This was because I dug my own grave with this code, but it should not\n",
    "    affect any results. \n",
    "    '''\n",
    "    if is_full_string:\n",
    "        return x if criterion(x) == 1 else None\n",
    "    if position is not None:\n",
    "        return x[position] if criterion(x) == 1 else None\n",
    "    subsets = []\n",
    "    for i in range(len(x)):\n",
    "        for j in range(i + 1, len(x) + 1):\n",
    "            if criterion(x[i:j]) == 1:\n",
    "                subsets.append(x[i:j])\n",
    "    if subsets == []:\n",
    "        return None\n",
    "    return subsets\n",
    "\n",
    "\n",
    "# Rubric to generate data that is `good' (in-phenomenon)\n",
    "def xor(a, b):\n",
    "    return a^b\n",
    "\n",
    "rubric_good = [\n",
    "    lambda x: 1 if x.count(\"0\") % 2 == 0 else 0, # Does it have an even number of zeros?\n",
    "    lambda x: 1 if xor(x[0] == \"0\", \"10101\" in x) else 0, # Does it start with a zero OR contain 10101?\n",
    "    lambda x: 1 if x.count(\"1\") > 5 else 0, # Is the number of ones larger than 5? (k-dependent!)\n",
    "]\n",
    "\n",
    "with_xors = {1: [lambda x: x[0] == \"0\", lambda x: \"10101\" in x]}\n",
    "\n",
    "# Rubric to generate data that is `bad' (out-of-phenomenon)\n",
    "rubric_other = [\n",
    "    lambda x: 1 if x.find(\"111\") != -1 else 0, # Does it contain three ones next to one another?\n",
    "    lambda x: 1 if x[-1] == \"1\" else 0, # Does it end with a 1?\n",
    "    lambda x: 1 if x.find(\"110001\") != -1 else 0 , # Does it contain either 110 and 001?\n",
    "]\n",
    "\n",
    "# Natural-language descriptions (which we will use in LLMs)\n",
    "good_rubric_nl = \"- If the string contains an even number of zeros, 1. Otherwise 0\\n\\\n",
    "    - If the string starts with a zero OR contains 10101 (but not both), it is 1. Otherwise 0\\n\\\n",
    "    - If the string has more than five ones, it is 1. Otherwise 0.\"\n",
    "\n",
    "other_rubric_nl = \"- If the string contains three consecutive ones, 1. Otherwise 0\\n\\\n",
    "    - If the string ends with a one, it is 1. Otherwise 0\\n\\\n",
    "    - If the string contains the substring 110001, it is 1. Otherwise 0.\"\n",
    "\n",
    "# Natural-language description of the aggregator, for the LLM.\n",
    "# While in theory it should NOT know this, it does seem like without \n",
    "# bprop it'll be nearly impossible to solve otherwise.\n",
    "aggregator_nl = \"The final label should be determined by majority vote.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "373d6827",
   "metadata": {},
   "source": [
    "## Generate data (or load it)\n",
    "\n",
    "We generate the dataset for reproducibility purposes, and then load it (hence why it is all commented out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "93ce5081",
   "metadata": {},
   "outputs": [],
   "source": [
    "#good_data = dataset_generator(n=2500, k=K, rubric=rubric_good, balance=True)\n",
    "#other_data = dataset_generator(n=2500, k=K, rubric=rubric_other, balance=True)\n",
    "#with open(\"good_data_train_with_xor.json\", \"w\", encoding=\"utf-8\") as f: json.dump(good_data[:2000], f)\n",
    "#with open(\"good_data_test_with_xor.json\", \"w\", encoding=\"utf-8\") as f: json.dump(good_data[2000:], f)\n",
    "#with open(\"other_data_train_with_xor.json\", \"w\", encoding=\"utf-8\") as f: json.dump(other_data[:2000], f)\n",
    "#with open(\"other_data_test_with_xor.json\", \"w\", encoding=\"utf-8\") as f: json.dump(other_data[2000:], f)\n",
    "\n",
    "good_data_train = json.load(open(\"good_data_train_with_xor.json\", \"r\", encoding=\"utf-8\"))\n",
    "good_data_test = json.load(open(\"good_data_test_with_xor.json\", \"r\", encoding=\"utf-8\"))\n",
    "other_data_train = json.load(open(\"other_data_train_with_xor.json\", \"r\", encoding=\"utf-8\"))\n",
    "other_data_test = json.load(open(\"other_data_test_with_xor.json\", \"r\", encoding=\"utf-8\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a59a934",
   "metadata": {},
   "source": [
    "## Algorithm and so on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3bfb0a89",
   "metadata": {},
   "outputs": [],
   "source": [
    "def no_data_algorithm(X, evaluator, generator, rubric, max_rounds=3,\n",
    "                      max_ev_rounds=1, noise=None):\n",
    "    '''\n",
    "    Baseline implementation of the no-data algorithm without flipping labels.\n",
    "    The generator parameter is because we will poll the LLM with its understanding\n",
    "    of the data (decision trees aren't really good at generating, so we'll use a random choice)\n",
    "    '''\n",
    "    labels = []\n",
    "    successes = []\n",
    "    for x in tqdm(X):\n",
    "        success, label, _ = ev_protocol(x, evaluator, generator, rubric,\n",
    "                                             max_rounds, noise)\n",
    "        if success:\n",
    "            labels.append(label)\n",
    "            successes.append(1)\n",
    "        else:\n",
    "            labels.append(not label)\n",
    "    return labels, successes\n",
    "\n",
    "\n",
    "def no_data_algorithm_with_flips(X, evaluator, generator, rubric, max_rounds=3,\n",
    "                                 phi=0.3, noise=None):\n",
    "    '''\n",
    "    The actual implementation of the No-Data algorithm, flipping labels.\n",
    "    The generator parameter is because we will poll the LLM with its understanding\n",
    "    of the data (decision trees aren't really good at generating, so we'll use a random choice)\n",
    "    '''\n",
    "    labels = []\n",
    "    flips = []\n",
    "    successes = []\n",
    "    for x in tqdm(X):\n",
    "\n",
    "        success, label, reason = ev_protocol(x, evaluator, generator, rubric, \n",
    "                                                max_rounds, noise)\n",
    "        if success:\n",
    "            flips.append([False, reason] if \"Random\" not in reason else [True, reason])\n",
    "            labels.append(label)\n",
    "            successes.append(1)\n",
    "        else:\n",
    "            if random.uniform(0, 1) < phi:\n",
    "                labels.append(label)\n",
    "                flips.append([False, \"Random flip\"])\n",
    "            else:\n",
    "                label = not label\n",
    "                labels.append(label)\n",
    "                flips.append([True, \"Random flip\"])\n",
    "\n",
    "    return labels, successes, flips\n",
    "\n",
    "\n",
    "def ev_protocol(x, evaluator, generator, rubric, max_rounds, noise):\n",
    "    ''' \n",
    "    Quick and dirty implementation of the EV Protocol. In here, \n",
    "    we understand lying as generating a datapoint that is not within\n",
    "    the correct phenomenon. \n",
    "    '''\n",
    "    for round in range(max_rounds):\n",
    "        # Step 1: generate x \\cong x' based on the belief of y\n",
    "        y_tilde = evaluator(x)\n",
    "        # Always generate with the rubric you know\n",
    "        x_tilde = generator(x, y_tilde, rubric_good)\n",
    "        # Step 2: get the challenge\n",
    "        check = random.choice([check1, check2])\n",
    "        # Step 3: git checkd\n",
    "        success, reason = check(x_tilde, x, rubric)\n",
    "        # Step 3.5: Randomness, if enabled, for automated accept\n",
    "        if noise is not None and success is False:\n",
    "            if random.uniform(0, 1) < noise:\n",
    "                return True, y_tilde, (\"Random guess\", x, x_tilde)\n",
    "        # Last bugfix: if the label is zero, the checks should fail... right?\n",
    "        if not success: break\n",
    "    return success, y_tilde, (reason, x, x_tilde)\n",
    "\n",
    "\n",
    "def check1(x_tilde, x, rubric):\n",
    "    ''' \n",
    "    Assert that a subset matching the criteria exists in both\n",
    "    x_tilde and x.\n",
    "    I.e., verify the isomorphism\n",
    "    '''\n",
    "    for i, c in enumerate(rubric):\n",
    "        if i in with_xors and int(rubric[0](\"0000\")) == 1:\n",
    "            c1, c1tilde = with_xors[i][0](x), with_xors[i][0](x_tilde)\n",
    "            c2, c2tilde = with_xors[i][1](x), with_xors[i][1](x_tilde)\n",
    "            if c1 != c1tilde:\n",
    "                return False, f\"Ch1 Failed iso {i}\"\n",
    "            if c2 != c2tilde:\n",
    "                return False, f\"Ch1 Failed iso {i}\"\n",
    "            if c(x) != c(x_tilde):\n",
    "                return False, f\"Ch1 Failed iso {i}\"\n",
    "        else:\n",
    "            if c(x) != c(x_tilde):\n",
    "                return False, f\"Ch1 Failed iso {i}\"\n",
    "\n",
    "    return True, \"\"\n",
    "\n",
    "\n",
    "def check2(x_tilde, x, rubric):\n",
    "    ''' \n",
    "    Assert that the output of the criteria matches in both cases\n",
    "    '''\n",
    "    x_tilde_encoding = \"\".join([str(c(x_tilde)) for c in rubric])\n",
    "    x_encoding = \"\".join([str(c(x)) for c in rubric])\n",
    "    if x_tilde_encoding == x_encoding:\n",
    "        return True, \"\"\n",
    "    return False, \"Ch2 Encoding\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7ef6d13",
   "metadata": {},
   "source": [
    "# Experiments - DT\n",
    "A good-old fashioned decision tree. This one is trained with the 'good' dataset and thus we can make a strong argument that it has neither seen the 'other' dataset nor memorised it.\n",
    "\n",
    "There'll be several sub-experiments to evaluate the generator (oracle, noisy, etc) as well as the evaluator (pretrained, lying, etc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32666643",
   "metadata": {},
   "source": [
    "This uses the `dt_generator_base`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e4df9ad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dt_generator_base(x, y=None, rubric=None):\n",
    "    ''' \n",
    "    Call our data generation function for a single datapoint, until\n",
    "    we find one that satisfies the rubric (whatever that is).\n",
    "    This is the base case (when it actually knows what it is talking about).\n",
    "    This one also is used to construct the exemplars in the LLM call\n",
    "    '''\n",
    "    # Note that we could have done something more sophisticated, like\n",
    "    # random indices, but the code would be more complicated (you will need)\n",
    "    # to check for isomorphism AND permutation\n",
    "    found = False\n",
    "    while not found:\n",
    "        x_tilde = \"\".join([random.choice([\"0\", \"1\"]) for _ in range(K)])\n",
    "        found = True\n",
    "        for i, c in enumerate(rubric):\n",
    "            if i in with_xors and int(rubric[0](\"0000\")) == 1:\n",
    "                c1, c1tilde = with_xors[i][0](x), with_xors[i][0](x_tilde)\n",
    "                c2, c2tilde = with_xors[i][1](x), with_xors[i][1](x_tilde)\n",
    "                if c1 != c1tilde:\n",
    "                    found = False\n",
    "                if c2 != c2tilde:\n",
    "                    found = False\n",
    "                if c(x) != c(x_tilde):\n",
    "                    found = False\n",
    "            else:\n",
    "                if c(x_tilde) != c(x):\n",
    "                    found = False\n",
    "    return x_tilde\n",
    "\n",
    "\n",
    "def dt_generator_probabilistic(x, y=None, rubric=None, p=0.25):\n",
    "    ''' \n",
    "    Call our data generation function for a single datapoint, until\n",
    "    we find one that satisfies the rubric (whatever that is).\n",
    "    Then, with probability 1/4, lie.\n",
    "    '''\n",
    "    # Note that we could have done something more sophisticated, like\n",
    "    # random indices, but the code would be more complicated (you will need)\n",
    "    # to check for isomorphism AND permutation\n",
    "    found = False\n",
    "    while not found:\n",
    "        x_tilde = \"\".join([random.choice([\"0\", \"1\"]) for _ in range(K)])\n",
    "        found = True\n",
    "        for i, c in enumerate(rubric):\n",
    "            if i in with_xors and int(rubric[0](\"0000\")) == 1:\n",
    "                c1, c1tilde = with_xors[i][0](x), with_xors[i][0](x_tilde)\n",
    "                c2, c2tilde = with_xors[i][1](x), with_xors[i][1](x_tilde)\n",
    "                if c1 != c1tilde:\n",
    "                    found = False\n",
    "                if c2 != c2tilde:\n",
    "                    found = False\n",
    "                if c(x) != c(x_tilde):\n",
    "                    found = False\n",
    "            else:\n",
    "                if c(x_tilde) != c(x):\n",
    "                    found = False\n",
    "    if random.uniform(0,1) < p:\n",
    "        return \"\".join([random.choice([\"0\", \"1\"]) for _ in range(K)])\n",
    "    return x_tilde\n",
    "\n",
    "\n",
    "def dt_generator_lying_no_f(x, y=None, rubric=None):\n",
    "    ''' \n",
    "    A lying generator (no f known)\n",
    "    This is equivalent to claiming (falsely) \"I understand the task\"\n",
    "    '''\n",
    "    found = False\n",
    "    while not found:\n",
    "        x_tilde = \"\".join([random.choice([\"0\", \"1\"]) for _ in range(K)])\n",
    "        y_tilde = get_label_for(x_tilde)\n",
    "        if int(y_tilde) == int(y): found = True\n",
    "    return x_tilde\n",
    "\n",
    "\n",
    "def dt_generator_lying_no_sigma(x, y=None, rubric=None):\n",
    "    ''' \n",
    "    A lying generator (no sigma known). This one is more sophisticated,\n",
    "    since it \"understands\" the rubric but it can't generate something that\n",
    "    is isomorphic, just similar.\n",
    "    It is equivalent to falsely claiming you know how to label the data \n",
    "    given the rubric. \n",
    "    '''\n",
    "    expected = \"\".join([str(c(x)) for c in rubric])\n",
    "    expected = ''.join(sorted(expected))\n",
    "    while True:\n",
    "        x_tilde = \"\".join([random.choice([\"0\", \"1\"]) for _ in range(K)])\n",
    "        prototype = [str(c(x_tilde)) for c in rubric]\n",
    "        prototype = ''.join(sorted(prototype))\n",
    "        if expected == prototype:\n",
    "            break\n",
    "    return x_tilde\n",
    "\n",
    "\n",
    "def dt_evaluator(x):\n",
    "    ''' \n",
    "    Wrapper to maintain signatures \n",
    "    '''\n",
    "    return int(clf.predict(np.array([x]).reshape(-1, 1))[0])\n",
    "\n",
    "\n",
    "def dt_evaluator_lying(x, p=0.1):\n",
    "    ''' \n",
    "    Wrapper to maintain signatures \n",
    "    '''\n",
    "    label = int(clf.predict(np.array([x]).reshape(-1, 1))[0])\n",
    "    if random.uniform(0, 1) < p:\n",
    "        return int(not label)\n",
    "    return label\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ca88015",
   "metadata": {},
   "source": [
    "## Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "bc012429",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In-phenomenon baseline test score: 0.622 | 0.598\n",
      "Out-of-phenomenon baseline test score: 0.542 | 54.217\n"
     ]
    }
   ],
   "source": [
    "# First we baseline\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "clf = DecisionTreeClassifier(random_state=123)\n",
    "\n",
    "X = np.array([p[0] for p in good_data_train])\n",
    "clf.fit(X.reshape(-1, 1), [p[-1] for p in good_data_train])\n",
    "\n",
    "X = np.array([p[0] for p in good_data_test])\n",
    "Y = [int(p[-1]) for p in good_data_test]\n",
    "preds = [int(k) for k in clf.predict(X.reshape(-1, 1))]\n",
    "dt_in_phenomenon_baseline_accuracy = round(accuracy_score(Y, preds), 3)\n",
    "dt_in_phenomenon_baseline_f1 = round(f1_score(Y, preds), 3)\n",
    "\n",
    "X = np.array([p[0] for p in other_data_test])\n",
    "Y = [int(p[-1]) for p in other_data_test]\n",
    "preds = [int(k) for k in clf.predict(X.reshape(-1, 1))]\n",
    "dt_out_phenomenon_baseline_accuracy = round(accuracy_score(Y, preds), 3)\n",
    "dt_out_phenomenon_baseline_f1 = round(f1_score(Y, preds)*100, 3)\n",
    "\n",
    "print(f\"In-phenomenon baseline test score: {dt_in_phenomenon_baseline_accuracy} | {dt_in_phenomenon_baseline_f1}\")\n",
    "print(f\"Out-of-phenomenon baseline test score: {dt_out_phenomenon_baseline_accuracy} | {dt_out_phenomenon_baseline_f1}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5f8789a",
   "metadata": {},
   "source": [
    "## The No-Data Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "5af3869b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 498/498 [00:00<00:00, 1816.34it/s]\n",
      "100%|██████████| 498/498 [00:00<00:00, 3309.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In-phenomenon test score: 0.622 | Successes: 100.0 | F1: 59.829 | Flips: 0.0\n",
      "Out-phenomenon test score: 0.528 | Successes: 4.819 | F1: 52.138 | Flips: 46.386\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "phi = 0.5 # Doesn't matter in ID since it is using the oracular generator\n",
    "good_labels, good_successes, good_flips = no_data_algorithm_with_flips([p[0] for p in good_data_test], \n",
    "                                                evaluator=dt_evaluator,\n",
    "                                                generator=dt_generator_base,\n",
    "                                                rubric=rubric_good,\n",
    "                                                max_rounds=rounds,\n",
    "                                                phi=phi, noise=None)\n",
    "\n",
    "\n",
    "other_labels, other_successes, other_flips = no_data_algorithm_with_flips([p[0] for p in other_data_test], \n",
    "                                                evaluator=dt_evaluator,\n",
    "                                                generator=dt_generator_base,\n",
    "                                                rubric=rubric_other,\n",
    "                                                max_rounds=rounds, \n",
    "                                                phi=phi, noise=None) # Check with the correct datapoint\n",
    "\n",
    "print_metrics(good_labels, good_successes, good_data_test, \"In\", good_flips)\n",
    "print_metrics(other_labels, other_successes, other_data_test, \"Out\", other_flips)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f5d55cd",
   "metadata": {},
   "source": [
    "## Ablation Studies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48270ef2",
   "metadata": {},
   "source": [
    "#### No flips - Lying Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c5150670",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 498/498 [00:00<00:00, 6569.51it/s]\n",
      "100%|██████████| 498/498 [00:00<00:00, 9029.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generator: no sigma\n",
      "In-phenomenon test score: 0.488 | Successes: 18.273 | F1: 51.243\n",
      "Out-phenomenon test score: 0.46 | Successes: 1.807 | F1: 46.307\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 498/498 [00:00<00:00, 13510.06it/s]\n",
      "100%|██████████| 498/498 [00:00<00:00, 8355.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generator: no f\n",
      "In-phenomenon test score: 0.382 | Successes: 0.402 | F1: 41.667\n",
      "Out-phenomenon test score: 0.476 | Successes: 2.61 | F1: 47.695\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 498/498 [00:00<00:00, 2335.46it/s]\n",
      "100%|██████████| 498/498 [00:00<00:00, 3597.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generator: p = 0.1\n",
      "In-phenomenon test score: 0.502 | Successes: 50.201 | F1: 50.4\n",
      "Out-phenomenon test score: 0.464 | Successes: 4.217 | F1: 46.493\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 498/498 [00:00<00:00, 1899.78it/s]\n",
      "100%|██████████| 498/498 [00:00<00:00, 4270.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generator: oracle\n",
      "In-phenomenon test score: 0.622 | Successes: 100.0 | F1: 59.829\n",
      "Out-phenomenon test score: 0.462 | Successes: 4.819 | F1: 45.082\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for generator_name, generator in [(\"no sigma\", dt_generator_lying_no_sigma), \n",
    "                                (\"no f\", dt_generator_lying_no_f),\n",
    "                                (\"p = 0.1\", dt_generator_probabilistic),\n",
    "                                (\"oracle\", dt_generator_base)]:\n",
    "\n",
    "    good_labels, good_successes = no_data_algorithm([p[0] for p in good_data_test], \n",
    "                                                    evaluator=dt_evaluator,\n",
    "                                                    generator=generator,\n",
    "                                                    rubric=rubric_good,\n",
    "                                                    max_rounds=rounds,\n",
    "                                                    noise=None)\n",
    "\n",
    "    other_labels, other_successes = no_data_algorithm([p[0] for p in other_data_test], \n",
    "                                                    evaluator=dt_evaluator,\n",
    "                                                    generator=generator,\n",
    "                                                    rubric=rubric_other,\n",
    "                                                    max_rounds=rounds,\n",
    "                                                    noise=None) # Check with the correct datapoint\n",
    "\n",
    "    print(f\"Generator: {generator_name}\")\n",
    "    print_metrics(good_labels, good_successes, good_data_test, \"In\")\n",
    "    print_metrics(other_labels, other_successes, other_data_test, \"Out\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ee991fc",
   "metadata": {},
   "source": [
    "#### Flips -- Lying Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "04daf8bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------- Phi: 0.0 ---------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 498/498 [00:00<00:00, 1473.43it/s]\n",
      "100%|██████████| 498/498 [00:00<00:00, 1943.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generator: no sigma\n",
      "In-phenomenon test score: 0.482 | Successes: 16.064 | F1: 50.763 | Flips: 83.936\n",
      "Out-phenomenon test score: 0.468 | Successes: 2.209 | F1: 46.894 | Flips: 97.791\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 498/498 [00:00<00:00, 2624.65it/s]\n",
      "100%|██████████| 498/498 [00:00<00:00, 2717.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generator: no f\n",
      "In-phenomenon test score: 0.376 | Successes: 0.602 | F1: 40.987 | Flips: 99.398\n",
      "Out-phenomenon test score: 0.484 | Successes: 4.217 | F1: 49.109 | Flips: 95.783\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 498/498 [00:00<00:00, 713.42it/s]\n",
      "100%|██████████| 498/498 [00:00<00:00, 1198.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generator: p = 0.1\n",
      "In-phenomenon test score: 0.484 | Successes: 47.59 | F1: 49.31 | Flips: 52.41\n",
      "Out-phenomenon test score: 0.468 | Successes: 4.618 | F1: 46.247 | Flips: 95.382\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 498/498 [00:00<00:00, 592.39it/s]\n",
      "100%|██████████| 498/498 [00:00<00:00, 1019.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generator: oracle\n",
      "In-phenomenon test score: 0.622 | Successes: 100.0 | F1: 59.829 | Flips: 0.0\n",
      "Out-phenomenon test score: 0.464 | Successes: 5.823 | F1: 45.842 | Flips: 94.177\n",
      "--------------- Phi: 0.3 ---------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 498/498 [00:00<00:00, 1845.26it/s]\n",
      "100%|██████████| 498/498 [00:00<00:00, 3339.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generator: no sigma\n",
      "In-phenomenon test score: 0.514 | Successes: 19.277 | F1: 54.682 | Flips: 60.643\n",
      "Out-phenomenon test score: 0.476 | Successes: 1.606 | F1: 48.521 | Flips: 69.679\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 498/498 [00:00<00:00, 3305.39it/s]\n",
      "100%|██████████| 498/498 [00:00<00:00, 3166.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generator: no f\n",
      "In-phenomenon test score: 0.472 | Successes: 0.602 | F1: 49.326 | Flips: 68.072\n",
      "Out-phenomenon test score: 0.52 | Successes: 2.008 | F1: 51.125 | Flips: 68.072\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 498/498 [00:00<00:00, 813.98it/s]\n",
      "100%|██████████| 498/498 [00:00<00:00, 1213.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generator: p = 0.1\n",
      "In-phenomenon test score: 0.508 | Successes: 46.386 | F1: 50.505 | Flips: 39.558\n",
      "Out-phenomenon test score: 0.478 | Successes: 2.61 | F1: 46.939 | Flips: 65.06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 498/498 [00:01<00:00, 489.90it/s]\n",
      "100%|██████████| 498/498 [00:00<00:00, 1684.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generator: oracle\n",
      "In-phenomenon test score: 0.622 | Successes: 100.0 | F1: 59.829 | Flips: 0.0\n",
      "Out-phenomenon test score: 0.496 | Successes: 4.618 | F1: 49.087 | Flips: 66.064\n",
      "--------------- Phi: 0.5 ---------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 498/498 [00:00<00:00, 2790.31it/s]\n",
      "100%|██████████| 498/498 [00:00<00:00, 3551.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generator: no sigma\n",
      "In-phenomenon test score: 0.542 | Successes: 19.277 | F1: 54.032 | Flips: 40.161\n",
      "Out-phenomenon test score: 0.484 | Successes: 1.004 | F1: 49.31 | Flips: 50.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 498/498 [00:00<00:00, 4254.65it/s]\n",
      "100%|██████████| 498/498 [00:00<00:00, 4302.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generator: no f\n",
      "In-phenomenon test score: 0.512 | Successes: 1.004 | F1: 50.909 | Flips: 47.992\n",
      "Out-phenomenon test score: 0.514 | Successes: 2.811 | F1: 52.174 | Flips: 48.996\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 498/498 [00:00<00:00, 729.92it/s] \n",
      "100%|██████████| 498/498 [00:00<00:00, 824.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generator: p = 0.1\n",
      "In-phenomenon test score: 0.544 | Successes: 47.791 | F1: 52.008 | Flips: 27.51\n",
      "Out-phenomenon test score: 0.522 | Successes: 2.811 | F1: 50.826 | Flips: 47.791\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 498/498 [00:00<00:00, 689.23it/s]\n",
      "100%|██████████| 498/498 [00:00<00:00, 1789.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generator: oracle\n",
      "In-phenomenon test score: 0.622 | Successes: 100.0 | F1: 59.829 | Flips: 0.0\n",
      "Out-phenomenon test score: 0.546 | Successes: 6.426 | F1: 55.159 | Flips: 47.791\n",
      "--------------- Phi: 0.6 ---------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 498/498 [00:00<00:00, 3097.67it/s]\n",
      "100%|██████████| 498/498 [00:00<00:00, 2826.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generator: no sigma\n",
      "In-phenomenon test score: 0.586 | Successes: 16.867 | F1: 58.8 | Flips: 32.53\n",
      "Out-phenomenon test score: 0.524 | Successes: 2.811 | F1: 53.801 | Flips: 35.141\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 498/498 [00:00<00:00, 3221.11it/s]\n",
      "100%|██████████| 498/498 [00:00<00:00, 1284.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generator: no f\n",
      "In-phenomenon test score: 0.506 | Successes: 0.602 | F1: 48.101 | Flips: 37.349\n",
      "Out-phenomenon test score: 0.514 | Successes: 3.012 | F1: 52.174 | Flips: 35.743\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 498/498 [00:00<00:00, 780.08it/s]\n",
      "100%|██████████| 498/498 [00:00<00:00, 1763.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generator: p = 0.1\n",
      "In-phenomenon test score: 0.558 | Successes: 43.574 | F1: 54.167 | Flips: 24.9\n",
      "Out-phenomenon test score: 0.476 | Successes: 4.418 | F1: 49.516 | Flips: 37.952\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 498/498 [00:00<00:00, 950.82it/s] \n",
      "100%|██████████| 498/498 [00:00<00:00, 1965.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generator: oracle\n",
      "In-phenomenon test score: 0.622 | Successes: 100.0 | F1: 59.829 | Flips: 0.0\n",
      "Out-phenomenon test score: 0.486 | Successes: 6.024 | F1: 47.325 | Flips: 35.341\n",
      "--------------- Phi: 0.9 ---------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 498/498 [00:00<00:00, 3294.75it/s]\n",
      "100%|██████████| 498/498 [00:00<00:00, 4277.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generator: no sigma\n",
      "In-phenomenon test score: 0.592 | Successes: 16.667 | F1: 57.263 | Flips: 9.036\n",
      "Out-phenomenon test score: 0.522 | Successes: 2.209 | F1: 52.778 | Flips: 10.843\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 498/498 [00:00<00:00, 4643.80it/s]\n",
      "100%|██████████| 498/498 [00:00<00:00, 3659.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generator: no f\n",
      "In-phenomenon test score: 0.588 | Successes: 1.205 | F1: 55.724 | Flips: 9.839\n",
      "Out-phenomenon test score: 0.532 | Successes: 3.213 | F1: 51.357 | Flips: 9.839\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 498/498 [00:00<00:00, 948.40it/s] \n",
      "100%|██████████| 498/498 [00:00<00:00, 1672.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generator: p = 0.1\n",
      "In-phenomenon test score: 0.612 | Successes: 47.189 | F1: 58.134 | Flips: 5.823\n",
      "Out-phenomenon test score: 0.53 | Successes: 3.414 | F1: 53.012 | Flips: 10.843\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 498/498 [00:00<00:00, 764.02it/s]\n",
      "100%|██████████| 498/498 [00:00<00:00, 1013.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generator: oracle\n",
      "In-phenomenon test score: 0.622 | Successes: 100.0 | F1: 59.829 | Flips: 0.0\n",
      "Out-phenomenon test score: 0.544 | Successes: 4.217 | F1: 54.691 | Flips: 11.446\n",
      "--------------- Phi: 1.0 ---------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 498/498 [00:00<00:00, 1773.24it/s]\n",
      "100%|██████████| 498/498 [00:00<00:00, 3170.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generator: no sigma\n",
      "In-phenomenon test score: 0.622 | Successes: 17.47 | F1: 59.829 | Flips: 0.0\n",
      "Out-phenomenon test score: 0.542 | Successes: 3.012 | F1: 54.217 | Flips: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 498/498 [00:00<00:00, 4981.16it/s]\n",
      "100%|██████████| 498/498 [00:00<00:00, 4978.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generator: no f\n",
      "In-phenomenon test score: 0.622 | Successes: 1.205 | F1: 59.829 | Flips: 0.0\n",
      "Out-phenomenon test score: 0.542 | Successes: 3.614 | F1: 54.217 | Flips: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 498/498 [00:00<00:00, 1390.72it/s]\n",
      "100%|██████████| 498/498 [00:00<00:00, 2542.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generator: p = 0.1\n",
      "In-phenomenon test score: 0.622 | Successes: 49.598 | F1: 59.829 | Flips: 0.0\n",
      "Out-phenomenon test score: 0.542 | Successes: 4.016 | F1: 54.217 | Flips: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 498/498 [00:00<00:00, 1340.61it/s]\n",
      "100%|██████████| 498/498 [00:00<00:00, 2343.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generator: oracle\n",
      "In-phenomenon test score: 0.622 | Successes: 100.0 | F1: 59.829 | Flips: 0.0\n",
      "Out-phenomenon test score: 0.542 | Successes: 5.422 | F1: 54.217 | Flips: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for phi in [0.0, 0.3, 0.5, 0.6, 0.9, 1.0]:\n",
    "    print(f\"--------------- Phi: {phi} ---------------\")\n",
    "    for generator_name, generator in [(\"no sigma\", dt_generator_lying_no_sigma), \n",
    "                                    (\"no f\", dt_generator_lying_no_f),\n",
    "                                    (\"p = 0.1\", dt_generator_probabilistic),\n",
    "                                    (\"oracle\", dt_generator_base)]:\n",
    "\n",
    "        good_labels, good_successes, good_flips = no_data_algorithm_with_flips([p[0] for p in good_data_test], \n",
    "                                                        evaluator=dt_evaluator,\n",
    "                                                        generator=generator,\n",
    "                                                        rubric=rubric_good,\n",
    "                                                        max_rounds=rounds,\n",
    "                                                        phi=phi, noise=None)\n",
    "\n",
    "        other_labels, other_successes, other_flips = no_data_algorithm_with_flips([p[0] for p in other_data_test], \n",
    "                                                        evaluator=dt_evaluator,\n",
    "                                                        generator=generator,\n",
    "                                                        rubric=rubric_other,\n",
    "                                                        max_rounds=rounds,\n",
    "                                                        phi = phi, noise=None) # Check with the correct datapoint\n",
    "\n",
    "        print(f\"Generator: {generator_name}\")\n",
    "        print_metrics(good_labels, good_successes, good_data_test, \"In\", good_flips)\n",
    "        print_metrics(other_labels, other_successes, other_data_test, \"Out\", other_flips)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e0e9b39",
   "metadata": {},
   "source": [
    "#### No Flips -- Lying Evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "4f93f991",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 498/498 [00:00<00:00, 4380.58it/s]\n",
      "100%|██████████| 498/498 [00:00<00:00, 4477.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generator: no sigma\n",
      "In-phenomenon test score: 0.426 | Successes: 15.06 | F1: 45.0\n",
      "Out-phenomenon test score: 0.49 | Successes: 2.209 | F1: 49.402\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 498/498 [00:00<00:00, 7767.40it/s]\n",
      "100%|██████████| 498/498 [00:00<00:00, 8795.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generator: no f\n",
      "In-phenomenon test score: 0.394 | Successes: 0.201 | F1: 42.366\n",
      "Out-phenomenon test score: 0.512 | Successes: 2.811 | F1: 52.816\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 498/498 [00:00<00:00, 1985.39it/s]\n",
      "100%|██████████| 498/498 [00:00<00:00, 3813.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generator: p = 0.1\n",
      "In-phenomenon test score: 0.49 | Successes: 51.205 | F1: 49.402\n",
      "Out-phenomenon test score: 0.466 | Successes: 4.618 | F1: 48.649\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 498/498 [00:00<00:00, 1582.13it/s]\n",
      "100%|██████████| 498/498 [00:00<00:00, 2214.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generator: oracle\n",
      "In-phenomenon test score: 0.59 | Successes: 100.0 | F1: 57.5\n",
      "Out-phenomenon test score: 0.456 | Successes: 6.426 | F1: 44.807\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "phi=0.5\n",
    "for generator_name, generator in [(\"no sigma\", dt_generator_lying_no_sigma), \n",
    "                                (\"no f\", dt_generator_lying_no_f),\n",
    "                                (\"p = 0.1\", dt_generator_probabilistic),\n",
    "                                (\"oracle\", dt_generator_base)]:\n",
    "\n",
    "    good_labels, good_successes = no_data_algorithm([p[0] for p in good_data_test], \n",
    "                                                    evaluator=dt_evaluator_lying,\n",
    "                                                    generator=generator,\n",
    "                                                    rubric=rubric_good,\n",
    "                                                    max_rounds=rounds,\n",
    "                                                     noise=None)\n",
    "\n",
    "    other_labels, other_successes = no_data_algorithm([p[0] for p in other_data_test], \n",
    "                                                    evaluator=dt_evaluator_lying,\n",
    "                                                    generator=generator,\n",
    "                                                    rubric=rubric_other,\n",
    "                                                    max_rounds=rounds,\n",
    "                                                    noise=None) # Check with the correct datapoint\n",
    "\n",
    "    print(f\"Generator: {generator_name}\")\n",
    "    print_metrics(good_labels, good_successes, good_data_test, \"In\")\n",
    "    print_metrics(other_labels, other_successes, other_data_test, \"Out\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef4f2634",
   "metadata": {},
   "source": [
    "#### Flips -- Lying Evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2b8ed633",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------- Phi: 0.1 -----------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 498/498 [00:00<00:00, 3590.98it/s]\n",
      "100%|██████████| 498/498 [00:00<00:00, 4637.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generator: no sigma\n",
      "In-phenomenon test score: 0.514 | Successes: 19.277 | F1: 52.918 | Flips: 71.888\n",
      "Out-phenomenon test score: 0.474 | Successes: 2.61 | F1: 46.091 | Flips: 84.94\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 498/498 [00:00<00:00, 5676.03it/s]\n",
      "100%|██████████| 498/498 [00:00<00:00, 5262.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generator: no f\n",
      "In-phenomenon test score: 0.452 | Successes: 0.602 | F1: 48.393 | Flips: 88.554\n",
      "Out-phenomenon test score: 0.462 | Successes: 2.008 | F1: 47.451 | Flips: 89.357\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 498/498 [00:00<00:00, 1445.48it/s]\n",
      "100%|██████████| 498/498 [00:00<00:00, 2587.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generator: p = 0.1\n",
      "In-phenomenon test score: 0.49 | Successes: 48.394 | F1: 48.163 | Flips: 46.787\n",
      "Out-phenomenon test score: 0.48 | Successes: 3.614 | F1: 48.915 | Flips: 82.932\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 498/498 [00:00<00:00, 1246.78it/s]\n",
      "100%|██████████| 498/498 [00:00<00:00, 2575.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generator: oracle\n",
      "In-phenomenon test score: 0.596 | Successes: 100.0 | F1: 57.862 | Flips: 0.0\n",
      "Out-phenomenon test score: 0.468 | Successes: 6.225 | F1: 47.106 | Flips: 84.337\n",
      "-------------------- Phi: 0.3 -----------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 498/498 [00:00<00:00, 3973.53it/s]\n",
      "100%|██████████| 498/498 [00:00<00:00, 4062.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generator: no sigma\n",
      "In-phenomenon test score: 0.492 | Successes: 16.667 | F1: 50.489 | Flips: 60.442\n",
      "Out-phenomenon test score: 0.532 | Successes: 2.811 | F1: 54.043 | Flips: 66.867\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 498/498 [00:00<00:00, 3618.51it/s]\n",
      "100%|██████████| 498/498 [00:00<00:00, 3779.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generator: no f\n",
      "In-phenomenon test score: 0.494 | Successes: 0.602 | F1: 50.588 | Flips: 68.072\n",
      "Out-phenomenon test score: 0.476 | Successes: 3.012 | F1: 47.485 | Flips: 68.474\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 498/498 [00:00<00:00, 1479.23it/s]\n",
      "100%|██████████| 498/498 [00:00<00:00, 2628.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generator: p = 0.1\n",
      "In-phenomenon test score: 0.5 | Successes: 50.201 | F1: 48.871 | Flips: 35.141\n",
      "Out-phenomenon test score: 0.474 | Successes: 2.41 | F1: 46.964 | Flips: 68.072\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 498/498 [00:00<00:00, 1364.00it/s]\n",
      "100%|██████████| 498/498 [00:00<00:00, 2996.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generator: oracle\n",
      "In-phenomenon test score: 0.59 | Successes: 100.0 | F1: 56.962 | Flips: 0.0\n",
      "Out-phenomenon test score: 0.474 | Successes: 4.418 | F1: 47.177 | Flips: 65.261\n",
      "-------------------- Phi: 0.5 -----------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 498/498 [00:00<00:00, 3320.92it/s]\n",
      "100%|██████████| 498/498 [00:00<00:00, 4198.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generator: no sigma\n",
      "In-phenomenon test score: 0.528 | Successes: 15.06 | F1: 51.745 | Flips: 39.357\n",
      "Out-phenomenon test score: 0.506 | Successes: 2.008 | F1: 50.8 | Flips: 47.992\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 498/498 [00:00<00:00, 4269.08it/s]\n",
      "100%|██████████| 498/498 [00:00<00:00, 4106.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generator: no f\n",
      "In-phenomenon test score: 0.496 | Successes: 0.602 | F1: 49.293 | Flips: 50.602\n",
      "Out-phenomenon test score: 0.51 | Successes: 1.807 | F1: 50.407 | Flips: 45.582\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 498/498 [00:00<00:00, 982.55it/s] \n",
      "100%|██████████| 498/498 [00:00<00:00, 1461.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generator: p = 0.1\n",
      "In-phenomenon test score: 0.522 | Successes: 48.594 | F1: 50.826 | Flips: 26.908\n",
      "Out-phenomenon test score: 0.51 | Successes: 3.012 | F1: 51.2 | Flips: 50.602\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 498/498 [00:00<00:00, 1006.39it/s]\n",
      "100%|██████████| 498/498 [00:00<00:00, 1973.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generator: oracle\n",
      "In-phenomenon test score: 0.588 | Successes: 100.0 | F1: 57.906 | Flips: 0.0\n",
      "Out-phenomenon test score: 0.492 | Successes: 3.614 | F1: 48.473 | Flips: 50.602\n",
      "-------------------- Phi: 0.6 -----------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 498/498 [00:00<00:00, 3546.27it/s]\n",
      "100%|██████████| 498/498 [00:00<00:00, 3895.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generator: no sigma\n",
      "In-phenomenon test score: 0.576 | Successes: 15.663 | F1: 56.495 | Flips: 32.329\n",
      "Out-phenomenon test score: 0.508 | Successes: 1.406 | F1: 50.704 | Flips: 41.165\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 498/498 [00:00<00:00, 5568.19it/s]\n",
      "100%|██████████| 498/498 [00:00<00:00, 4415.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generator: no f\n",
      "In-phenomenon test score: 0.49 | Successes: 0.201 | F1: 50.196 | Flips: 38.755\n",
      "Out-phenomenon test score: 0.484 | Successes: 1.606 | F1: 46.122 | Flips: 41.968\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 498/498 [00:00<00:00, 1285.88it/s]\n",
      "100%|██████████| 498/498 [00:00<00:00, 2776.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generator: p = 0.1\n",
      "In-phenomenon test score: 0.59 | Successes: 44.98 | F1: 58.367 | Flips: 20.683\n",
      "Out-phenomenon test score: 0.506 | Successes: 3.213 | F1: 48.101 | Flips: 37.751\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 498/498 [00:00<00:00, 1319.71it/s]\n",
      "100%|██████████| 498/498 [00:00<00:00, 2596.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generator: oracle\n",
      "In-phenomenon test score: 0.606 | Successes: 100.0 | F1: 58.824 | Flips: 0.0\n",
      "Out-phenomenon test score: 0.536 | Successes: 5.823 | F1: 54.076 | Flips: 37.149\n",
      "-------------------- Phi: 0.9 -----------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 498/498 [00:00<00:00, 4255.34it/s]\n",
      "100%|██████████| 498/498 [00:00<00:00, 5648.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generator: no sigma\n",
      "In-phenomenon test score: 0.588 | Successes: 16.265 | F1: 57.906 | Flips: 8.635\n",
      "Out-phenomenon test score: 0.514 | Successes: 2.008 | F1: 51.984 | Flips: 11.044\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 498/498 [00:00<00:00, 6739.88it/s]\n",
      "100%|██████████| 498/498 [00:00<00:00, 5520.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generator: no f\n",
      "In-phenomenon test score: 0.574 | Successes: 0.602 | F1: 55.833 | Flips: 11.245\n",
      "Out-phenomenon test score: 0.526 | Successes: 2.61 | F1: 51.639 | Flips: 7.631\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 498/498 [00:00<00:00, 1021.77it/s]\n",
      "100%|██████████| 498/498 [00:00<00:00, 2182.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generator: p = 0.1\n",
      "In-phenomenon test score: 0.586 | Successes: 47.791 | F1: 56.723 | Flips: 6.225\n",
      "Out-phenomenon test score: 0.532 | Successes: 2.811 | F1: 52.738 | Flips: 7.229\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 498/498 [00:00<00:00, 786.02it/s]\n",
      "100%|██████████| 498/498 [00:00<00:00, 1769.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generator: oracle\n",
      "In-phenomenon test score: 0.582 | Successes: 100.0 | F1: 55.932 | Flips: 0.0\n",
      "Out-phenomenon test score: 0.54 | Successes: 6.426 | F1: 52.784 | Flips: 9.639\n",
      "-------------------- Phi: 1.0 -----------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 498/498 [00:00<00:00, 2825.32it/s]\n",
      "100%|██████████| 498/498 [00:00<00:00, 2846.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generator: no sigma\n",
      "In-phenomenon test score: 0.622 | Successes: 18.876 | F1: 61.157 | Flips: 0.0\n",
      "Out-phenomenon test score: 0.548 | Successes: 1.807 | F1: 53.988 | Flips: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 498/498 [00:00<00:00, 4337.19it/s]\n",
      "100%|██████████| 498/498 [00:00<00:00, 2286.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generator: no f\n",
      "In-phenomenon test score: 0.586 | Successes: 0.201 | F1: 57.787 | Flips: 0.0\n",
      "Out-phenomenon test score: 0.502 | Successes: 3.213 | F1: 48.971 | Flips: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 498/498 [00:00<00:00, 674.88it/s]\n",
      "100%|██████████| 498/498 [00:00<00:00, 1645.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generator: p = 0.1\n",
      "In-phenomenon test score: 0.598 | Successes: 52.008 | F1: 57.265 | Flips: 0.0\n",
      "Out-phenomenon test score: 0.544 | Successes: 2.61 | F1: 54.326 | Flips: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 498/498 [00:00<00:00, 551.00it/s]\n",
      "100%|██████████| 498/498 [00:00<00:00, 1394.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generator: oracle\n",
      "In-phenomenon test score: 0.582 | Successes: 100.0 | F1: 55.172 | Flips: 0.0\n",
      "Out-phenomenon test score: 0.534 | Successes: 4.618 | F1: 54.15 | Flips: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for phi in [0.1, 0.3, 0.5, 0.6, 0.9, 1.0]:\n",
    "    print(f\"-------------------- Phi: {phi} -----------------\")\n",
    "    for generator_name, generator in [(\"no sigma\", dt_generator_lying_no_sigma), \n",
    "                                    (\"no f\", dt_generator_lying_no_f),\n",
    "                                    (\"p = 0.1\", dt_generator_probabilistic),\n",
    "                                    (\"oracle\", dt_generator_base)]:\n",
    "\n",
    "        good_labels, good_successes, good_flips = no_data_algorithm_with_flips([p[0] for p in good_data_test], \n",
    "                                                        evaluator=dt_evaluator_lying,\n",
    "                                                        generator=generator,\n",
    "                                                        rubric=rubric_good,\n",
    "                                                        max_rounds=rounds,\n",
    "                                                        phi=phi, noise=None)\n",
    "\n",
    "        other_labels, other_successes, other_flips = no_data_algorithm_with_flips([p[0] for p in other_data_test], \n",
    "                                                        evaluator=dt_evaluator_lying,\n",
    "                                                        generator=generator,\n",
    "                                                        rubric=rubric_other,\n",
    "                                                        max_rounds=rounds,\n",
    "                                                        phi = phi, noise=None) # Check with the correct datapoint\n",
    "\n",
    "        print(f\"Generator: {generator_name}\")\n",
    "        print_metrics(good_labels, good_successes, good_data_test, \"In\", good_flips)\n",
    "        print_metrics(other_labels, other_successes, other_data_test, \"Out\", other_flips)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "519d03a7",
   "metadata": {},
   "source": [
    "# Experiment 2: LLM Tests\n",
    "1. Baselines are GPT-4o and o3-mini\n",
    "2. Includes ablation study with the standard generator vs the picker\n",
    "\n",
    "The LLM Client we are using supports SLMs that are open, and _techincally_ also the closed AI models (like Open AI models). \n",
    "However, you'll have to bring in your own subscription. For these cases just modify `llmclient.py` to suit your needs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9239cf47",
   "metadata": {},
   "source": [
    "### LLM parameters, generators, etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aebb13c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llmclient import LLMClient, get_llm_response\n",
    "import re\n",
    "\n",
    "def build_synthetic_rationale_eval(x, rubric, rubric_nl):\n",
    "    '''\n",
    "    Automatically generate the chain-of-thought for the label.\n",
    "    What, you thought I'd just randomly prompt it and call it science? \n",
    "    '''\n",
    "    reasons = \"\"\n",
    "    values = []\n",
    "    for i, (c, _d) in enumerate(zip(rubric, rubric_nl.split(\"\\n\"))):\n",
    "        d = _d.split(\".\")[0].replace(\"-\", \"\").strip().lower()\n",
    "        is_full_string = False\n",
    "        if i == 0 and rubric[0](\"0000\") == 1:\n",
    "            is_full_string = True\n",
    "        position = None\n",
    "        if i == 1:\n",
    "            position = 0 if rubric[i](\"010\") == 1 else -1\n",
    "        subsets = what_matched_what(x, c, position=position, is_full_string=is_full_string)\n",
    "        if subsets is None or subsets == []:\n",
    "            reasons += f\"- There are no matches for {d}, so we make sure to not add any to our final string.\\n\"\n",
    "            values.append(\"0\")\n",
    "        else:\n",
    "            if is_full_string:\n",
    "                reasons += f\"- The string fully matches {d}, so the value is 1.\\n\"\n",
    "            elif position:\n",
    "                reasons += f\"- There is a match for {d} in the string at {position}, so the value is 1.\\n\"\n",
    "            else:\n",
    "                reasons += f\"- There is a match for {d} in the string: {subsets[0]}, so the value is 1.\\n\"\n",
    "            values.append(\"1\")\n",
    "    final_label = get_label_for(\"\".join(values))\n",
    "    reasons += f\"- Hence the final label is {final_label}\"\n",
    "    return reasons\n",
    "\n",
    "\n",
    "def get_evaluator_prompt(x, num_exemplars, data, use_other=False, include_aggregator=True):\n",
    "    '''\n",
    "    Evaluator prompt. Honestly there was more writing here than in the decision tree...\n",
    "    '''\n",
    "    rubric_nl = good_rubric_nl if not use_other else other_rubric_nl\n",
    "    rubric = rubric_good if not use_other else rubric_other\n",
    "\n",
    "    system_prompt = \"You are labelling binary strings based on a rubric (given below).\\n\\\n",
    "    First return the parts of the criterion that match the string, and the values.\\n\"\n",
    "    if include_aggregator:\n",
    "        system_prompt += \"Then return the label based on the aggregate function.\\n\"\n",
    "    system_prompt += f\"\\n# Rubric: {rubric_nl}\\n\"\n",
    "    if include_aggregator:\n",
    "        system_prompt += \"\\n# Aggregation Function:\\n{aggregator_nl}\\n\"\n",
    "    system_prompt += \"Return your answer in the form:\\n|reasons|\\n(list of reasons)\\n|reasons|\\n|label|\\n(the label)\\n|label|\"\n",
    "\n",
    "    exemplars = []\n",
    "    for n in range(num_exemplars):\n",
    "        x_data, y_data = data[n]\n",
    "        reasons = build_synthetic_rationale_eval(x_data, rubric, rubric_nl)\n",
    "        ex = f\"|reasons|\\n{reasons}\\n|reasons|\\n\"\n",
    "        ex += f\"|label|\\n{y_data}\\n|label|\"\n",
    "        exemplars.append({\"role\": \"user\", \"content\": x_data})\n",
    "        exemplars.append({\"role\": \"assistant\", \"content\": ex})\n",
    "\n",
    "    prompt = [{\"role\": \"system\", \"content\": system_prompt}]\n",
    "    prompt += exemplars\n",
    "    prompt += [{\"role\": \"user\", \"content\": x}]\n",
    "    return prompt\n",
    "\n",
    "\n",
    "def build_synthetic_rationale_generation(index, rubric_nl):\n",
    "    '''\n",
    "    Automatically generate the chain-of-thought for the label.\n",
    "    ---\n",
    "    1) If the string contains an even number of zeros, 1.\n",
    "    2) If the string starts with a zero OR contains 10101 (but not both), it is 1.\n",
    "    3) If the string has more than five ones, it is 1.\n",
    "    '''\n",
    "    map_to_wrongs = [\n",
    "        ['111001111100', ('1', '101')],\n",
    "        ['001000001001', ('0', '010')],\n",
    "        ['011010000100', ('1', '110')],\n",
    "        ['010111101111', ('1', '011')],\n",
    "        ['110010001010', ('0', '000')]\n",
    "    ]\n",
    "    x, tmp_cc_data = map_to_wrongs[index]\n",
    "    y, cc_data = tmp_cc_data\n",
    "\n",
    "    running_string = \"0100\"\n",
    "    reasons = f\"Our starting datapoint is {running_string}, as always.\\n\"\n",
    "\n",
    "    rubric_entries = rubric_nl.split(\"\\n\")\n",
    "    for i, value in enumerate(cc_data): \n",
    "        entry = rubric_entries[i]\n",
    "        d = entry.split(\".\")[0].replace(\"-\", \"\").strip().lower()\n",
    "        kth = [\"first\", \"second\", \"third\"][i]\n",
    "        reasons += f\"We now look at the {kth} criterion. The criterion is {d}.\\n\"\n",
    "        boilerplate = \"Since it is a match, we move on to the next criterion.\\n\"\n",
    "\n",
    "        if i == 0:\n",
    "            reasons += f\"The value for {x} in this criterion is {value}, because it has an {'even' if value == 1 else 'odd'} number of zeros.\\n\"\n",
    "            reasons += f\"Our datapoint ({running_string}) has 3 zeros, which is odd. \"\n",
    "            if value == 1:\n",
    "                reasons += boilerplate\n",
    "            else:\n",
    "                reasons += f\"We add a new zero at the end to avoid conflicts with the other criteria; and we get {running_string}0.\\n\"\n",
    "                running_string += \"0\"\n",
    "        elif i == 1:\n",
    "            reasons += f\"The value for {x} in this criterion is {value}, because it {'starts' if value == 1 else 'does not start'} with a zero.\\n\"\n",
    "            reasons += f\"Moreover, it {'does not contain' if '10101' not in x else 'contains'} the pattern 10101.\\n\"\n",
    "            # Triggered when neither or both patterns are in x.\n",
    "            # However, x_tilde does NOT have one of the patterns, so we need to fix this.\n",
    "            xor_triggered = False\n",
    "            if x[0] == \"0\" and \"10101\" in x:\n",
    "                xor_triggered = True\n",
    "            if x[0] != \"0\" and \"10101\" not in x:\n",
    "                xor_triggered = True\n",
    "                \n",
    "            if not xor_triggered:\n",
    "                reasons += \"The value is because only one of the tests is in the criteria.\"\n",
    "                if \"10101\" in x:\n",
    "                    reasons += f\"Our datapoint ({running_string}) starts with a zero, and does not have the pattern. The datapoint, however, has it backwards.\\n\"\n",
    "                    running_string = \"1\" + running_string[1:]\n",
    "                    reasons += f\"We flip the first bit to match it: {running_string}, \"\n",
    "                    running_string += \"10101\"\n",
    "                    reasons += f\"and add the pattern to it: {running_string}.\\n\"\n",
    "                else:\n",
    "                    reasons += f\"Our datapoint ({running_string}) starts with a zero, and does not have the pattern. This matches the datapoint.\\n\"\n",
    "                    reasons += boilerplate\n",
    "            else:\n",
    "                if x[0] != \"0\":\n",
    "                    reasons += f\"Our datapoint ({running_string}) starts with a zero and does not have the pattern.\"\n",
    "                    running_string = \"1\" + running_string[1:]\n",
    "                    reasons += f\"We then flip the first bit to match it: {running_string}.\\n\"\n",
    "                else:\n",
    "                    reasons += \"Since our string already starts with a zero, we need to add the pattern to ensure that it matches.\\n\"\n",
    "                    running_string += \"10101\"\n",
    "                    reasons += f\"Our string is now {running_string}.\\n\"\n",
    "        elif i == 2:\n",
    "            reasons += f\"The value for the datapoint in this criterion is {value}, since it has {x.count('1')} {\"(less than or equal to)\" if value == 1 else \"(at most)\"} five ones.\\n\"\n",
    "            reasons += f\"Our datapoint ({running_string}) has {running_string.count('1')} ones. \"\n",
    "            if value == 1:\n",
    "                reasons += \"Both strings have at most five ones, so we are finished.\\n\"\n",
    "            else:\n",
    "                nzeros = running_string.count('0')\n",
    "                flips_needed = nzeros - 5 if value == 1 else 5 - nzeros + 1\n",
    "                reasons += f\"We have {nzeros} zeros, so we need to flip at least {flips_needed} zeros. We focus on the inside of the string, to not break criterions 1 and 2.\\n\"\n",
    "                zindices = [j for j, c in enumerate(running_string) if c == '0'][:flips_needed]\n",
    "                new_string = [c if j not in zindices else str(int(not c)) for j, c in enumerate(running_string)]\n",
    "                running_string = \"\".join(new_string)\n",
    "                reasons += f\"So we now have {running_string}.\\n\"\n",
    "    reasons += f\"Hence our final string is:\\n|datapoint|\\n{running_string}\\n|datapoint|\"\n",
    "    return x, y, reasons\n",
    "\n",
    "\n",
    "def get_generator_prompt(x, y, k, num_exemplars, data, use_other=False):\n",
    "    '''\n",
    "    Generator prompt, generating a new x-tilde based on the rubric.\n",
    "    Note: the experiments never use `use_other`\n",
    "    '''\n",
    "    rubric_nl = good_rubric_nl if not use_other else other_rubric_nl\n",
    "    rubric = rubric_good if not use_other else rubric_other\n",
    "\n",
    "    system_prompt = f\"You are a datapoint generator over binary strings.\\n\\\n",
    "        Given a rubric (given below), a datapoint, and a label, return a similar datapoint that has the same label, and fulfils the same conditions as the rubric.\\n\\\n",
    "        For convenience, always start with the same datapoint: 0100. It will be easier to work with.\\n\"\n",
    "    # First analyse the datapoint based on the rubric and then return a similar datapoint.\\n\"\n",
    "    system_prompt += f\"\\n# Rubric: {rubric_nl}\\n\"\n",
    "    system_prompt += \"Return your rationale, and then the final datapoint in the form:\\n|datapoint|\\n(the datapoint)\\n|datapoint|\"\n",
    "\n",
    "    exemplars = []\n",
    "    for n in range(num_exemplars):\n",
    "        x_data, y_data, reasons = build_synthetic_rationale_generation(n, rubric_nl)\n",
    "        exemplars.append({\"role\": \"user\", \"content\": f\"Datapoint: {x_data}\\nLabel: {y_data}\"})\n",
    "        exemplars.append({\"role\": \"assistant\", \"content\": reasons})\n",
    "\n",
    "    prompt = [{\"role\": \"system\", \"content\": system_prompt}]\n",
    "    prompt += exemplars\n",
    "    prompt += [{\"role\": \"user\", \"content\": f\"Datapoint: {x}\\nLabel: {y}\"}]\n",
    "    return prompt\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "49ae4a33",
   "metadata": {},
   "outputs": [],
   "source": [
    "def llm_generator(x, y, rubric, data=good_data_train, num_exemplars=5):\n",
    "    ''' \n",
    "    LLM version of the data generation, which involves a call.\n",
    "    It does not need the rubric (always rubric_good) because it is already baked in.\n",
    "    '''\n",
    "    def parse_response(_r):\n",
    "        r = _r.split(\"|datapoint|\")\n",
    "        if len(r) > 1:\n",
    "            r = r[-2]\n",
    "            r = r.replace(\"|datapoint|\", \"\").strip()\n",
    "        else:\n",
    "            x = re.findall('[0|1].+', r[0])[-1]\n",
    "            r = x.replace(\"|\", \"\").strip()\n",
    "        if not all(c in '01' for c in r): \n",
    "            return None\n",
    "        return r\n",
    "\n",
    "    prompt = get_generator_prompt(x, y, k=K, num_exemplars=num_exemplars, \n",
    "                                  data=data)\n",
    "    resp, _resp = None, None\n",
    "    max_retries = 5\n",
    "    trial = 0\n",
    "    while resp is None:\n",
    "        if trial > max_retries: break\n",
    "        _resp = get_llm_response(llm, prompt)\n",
    "        try:\n",
    "            resp = parse_response(_resp)\n",
    "            with open(\"o3_log_generator.json\", \"a\", encoding=\"utf-8\") as f:\n",
    "                f.write(json.dumps({\"raw_resp\": _resp, \n",
    "                                    \"parse\": resp, \n",
    "                                    \"eval\": \"\".join([str(c(resp)) for c in rubric_good]),\n",
    "                                    \"eval_x\": \"\".join([str(c(x)) for c in rubric_good])}) + \"\\n\")\n",
    "        except:\n",
    "            resp = None\n",
    "            trial += 1\n",
    "    if trial > 0: \n",
    "        with open(\"o3_generator_parse_failure_logs.tsv\", \"a\", encoding=\"utf-8\") as f:\n",
    "            f.write(f\"{trial}\\t{_resp}\\n\") #\\t{''.join([str(c(resp)) for c in rubric_good])}\\n\")\n",
    "    if resp is None or resp == \"\":\n",
    "        resp = \"\".join([random.choice([\"0\", \"1\"]) for _ in range(K)])\n",
    "    return resp\n",
    "\n",
    "\n",
    "def llm_evaluator(x, data=good_data_train, num_exemplars=5):\n",
    "    ''' \n",
    "    Wrapper to maintain signatures \n",
    "    '''\n",
    "    def parse_response(_r):\n",
    "        r = _r.split(\"|reasons|\")[-1]\n",
    "        r = r.replace(\"|label|\", \"\").strip()\n",
    "        return int(r)\n",
    "\n",
    "    prompt = get_evaluator_prompt(x, num_exemplars, data)\n",
    "    resp = None\n",
    "    max_retries = 10\n",
    "    trial = 0 \n",
    "    while resp is None:\n",
    "        if trial > max_retries: break\n",
    "        _resp = get_llm_response(llm, prompt)\n",
    "        try:\n",
    "            resp = parse_response(_resp)\n",
    "        except:\n",
    "            resp = None\n",
    "            trial += 1\n",
    "    if resp is None: return random.choice([0, 1])\n",
    "    return resp\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a3f24e5",
   "metadata": {},
   "source": [
    "### Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "39d38bea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 498/498 [28:17<00:00,  3.41s/it]\n",
      "100%|██████████| 498/498 [20:48<00:00,  2.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In-phenomenon baseline test score: 0.61 | 0.697\n",
      "Out-of-phenomenon baseline test score: 0.558 | 0.664\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# First we baseline (omni)\n",
    "MODEL = \"gpt-4o-2024-05-13\"\n",
    "params = {\"max_tokens\": 1024, \"temperature\": 0.0} \n",
    "llm = LLMClient(params, MODEL)\n",
    "\n",
    "\n",
    "preds = []\n",
    "for pt in tqdm(good_data_test):\n",
    "    x = pt[0]\n",
    "    y = llm_evaluator(x, good_data_train, 5)\n",
    "    preds.append(int(y))\n",
    "\n",
    "Y = [int(p[-1]) for p in good_data_test]\n",
    "gpt4o_in_phenomenon_baseline_accuracy = round(accuracy_score(Y, preds), 3)\n",
    "gpt4o_in_phenomenon_baseline_f1 = round(f1_score(Y, preds), 3)\n",
    "\n",
    "preds = []\n",
    "for pt in tqdm(other_data_test):\n",
    "    x = pt[0]\n",
    "    y = llm_evaluator(x, good_data_train, 5) # the model only knows good\n",
    "    preds.append(int(y))\n",
    "\n",
    "Y = [int(p[-1]) for p in other_data_test]\n",
    "gpt4o_out_phenomenon_baseline_accuracy = round(accuracy_score(Y, preds), 3)\n",
    "gpt4o_out_phenomenon_baseline_f1 = round(f1_score(Y, preds), 3)\n",
    "\n",
    "print(f\"In-phenomenon baseline test score: {gpt4o_in_phenomenon_baseline_accuracy} | {gpt4o_in_phenomenon_baseline_f1}\")\n",
    "print(f\"Out-of-phenomenon baseline test score: {gpt4o_out_phenomenon_baseline_accuracy} | {gpt4o_out_phenomenon_baseline_f1}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "904a6de6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 498/498 [2:20:42<00:00, 16.95s/it]  \n",
      "100%|██████████| 498/498 [2:14:21<00:00, 16.19s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In-phenomenon baseline test score: 0.998 | 0.998\n",
      "Out-of-phenomenon baseline test score: 0.606 | 0.662\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# First we baseline (o3-mini)\n",
    "MODEL = \"gpt-o3-mini\"\n",
    "params = {\"max_completion_tokens\": 10000} #, \"temperature\": 0.0} \n",
    "llm = LLMClient(params, MODEL)\n",
    "\n",
    "preds = []\n",
    "for pt in tqdm(good_data_test):\n",
    "    x = pt[0]\n",
    "    y = llm_evaluator(x, good_data_train, 5)\n",
    "    preds.append(int(y))\n",
    "\n",
    "Y = [int(p[-1]) for p in good_data_test]\n",
    "gpt4o_in_phenomenon_baseline_accuracy = round(accuracy_score(Y, preds), 3)\n",
    "gpt4o_in_phenomenon_baseline_f1 = round(f1_score(Y, preds), 3)\n",
    "\n",
    "preds = []\n",
    "for pt in tqdm(other_data_test):\n",
    "    x = pt[0]\n",
    "    y = llm_evaluator(x, good_data_train, 5) # the model only knows good\n",
    "    preds.append(int(y))\n",
    "\n",
    "Y = [int(p[-1]) for p in other_data_test]\n",
    "gpt4o_out_phenomenon_baseline_accuracy = round(accuracy_score(Y, preds), 3)\n",
    "gpt4o_out_phenomenon_baseline_f1 = round(f1_score(Y, preds), 3)\n",
    "\n",
    "print(f\"In-phenomenon baseline test score: {gpt4o_in_phenomenon_baseline_accuracy} | {gpt4o_in_phenomenon_baseline_f1}\")\n",
    "print(f\"Out-of-phenomenon baseline test score: {gpt4o_out_phenomenon_baseline_accuracy} | {gpt4o_out_phenomenon_baseline_f1}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3becefe9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 498/498 [4:56:54<00:00, 35.77s/it]    \n",
      "100%|██████████| 498/498 [5:28:04<00:00, 39.53s/it]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In-phenomenon baseline test score: 0.61 | 0.708\n",
      "Out-of-phenomenon baseline test score: 0.544 | 0.656\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# First we baseline (DeepSeek)\n",
    "MODEL = \"deepseek-r1-distill-qwen-32b\"\n",
    "params = {\"max_tokens\": 2048, \"temperature\": 0.0} \n",
    "llm = LLMClient(params, MODEL)\n",
    "\n",
    "preds = []\n",
    "for pt in tqdm(good_data_test):\n",
    "    x = pt[0]\n",
    "    y = llm_evaluator(x, good_data_train, 5)\n",
    "    preds.append(int(y))\n",
    "\n",
    "Y = [int(p[-1]) for p in good_data_test]\n",
    "gpt4o_in_phenomenon_baseline_accuracy = round(accuracy_score(Y, preds), 3)\n",
    "gpt4o_in_phenomenon_baseline_f1 = round(f1_score(Y, preds), 3)\n",
    "\n",
    "preds = []\n",
    "for pt in tqdm(other_data_test):\n",
    "    x = pt[0]\n",
    "    y = llm_evaluator(x, good_data_train, 5) # the model only knows good\n",
    "    preds.append(int(y))\n",
    "\n",
    "Y = [int(p[-1]) for p in other_data_test]\n",
    "gpt4o_out_phenomenon_baseline_accuracy = round(accuracy_score(Y, preds), 3)\n",
    "gpt4o_out_phenomenon_baseline_f1 = round(f1_score(Y, preds), 3)\n",
    "\n",
    "print(f\"In-phenomenon baseline test score: {gpt4o_in_phenomenon_baseline_accuracy} | {gpt4o_in_phenomenon_baseline_f1}\")\n",
    "print(f\"Out-of-phenomenon baseline test score: {gpt4o_out_phenomenon_baseline_accuracy} | {gpt4o_out_phenomenon_baseline_f1}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ab7ff513",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 498/498 [14:37<00:00,  1.76s/it]\n",
      "100%|██████████| 498/498 [14:29<00:00,  1.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In-phenomenon baseline test score: 0.5 | 0.667\n",
      "Out-of-phenomenon baseline test score: 0.502 | 0.668\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# First we baseline (qwen-25-vl7b)\n",
    "MODEL = \"qwen-25-vl7b\"\n",
    "params = {\"max_tokens\": 1024, \"temperature\": 0.0} \n",
    "llm = LLMClient(params, MODEL)\n",
    "\n",
    "preds = []\n",
    "for pt in tqdm(good_data_test):\n",
    "    x = pt[0]\n",
    "    y = llm_evaluator(x, good_data_train, 5)\n",
    "    preds.append(int(y))\n",
    "\n",
    "Y = [int(p[-1]) for p in good_data_test]\n",
    "gpt4o_in_phenomenon_baseline_accuracy = round(accuracy_score(Y, preds), 3)\n",
    "gpt4o_in_phenomenon_baseline_f1 = round(f1_score(Y, preds), 3)\n",
    "\n",
    "preds = []\n",
    "for pt in tqdm(other_data_test):\n",
    "    x = pt[0]\n",
    "    y = llm_evaluator(x, good_data_train, 5) # the model only knows good\n",
    "    preds.append(int(y))\n",
    "\n",
    "Y = [int(p[-1]) for p in other_data_test]\n",
    "gpt4o_out_phenomenon_baseline_accuracy = round(accuracy_score(Y, preds), 3)\n",
    "gpt4o_out_phenomenon_baseline_f1 = round(f1_score(Y, preds), 3)\n",
    "\n",
    "print(f\"In-phenomenon baseline test score: {gpt4o_in_phenomenon_baseline_accuracy} | {gpt4o_in_phenomenon_baseline_f1}\")\n",
    "print(f\"Out-of-phenomenon baseline test score: {gpt4o_out_phenomenon_baseline_accuracy} | {gpt4o_out_phenomenon_baseline_f1}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1abbff81",
   "metadata": {},
   "source": [
    "## The ND Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27006b60",
   "metadata": {},
   "source": [
    "o3-mini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d6c3de49",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 498/498 [15:43:34<00:00, 113.68s/it]   \n",
      "100%|██████████| 498/498 [8:18:26<00:00, 60.05s/it]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In-phenomenon test score: 0.805 | Successes: 81.124 | F1: 78.775 | Flips: 18.876\n",
      "Out-phenomenon test score: 0.492 | Successes: 27.711 | F1: 43.4 | Flips: 72.289\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "phi = 0.0 #o3-mini, picker\n",
    "MODEL = \"gpt-o3-mini\"\n",
    "params = {\"max_completion_tokens\": 50000} #, \"temperature\": 0.0} \n",
    "llm = LLMClient(params, MODEL)\n",
    "\n",
    "good_labels, good_successes, good_flips = no_data_algorithm_with_flips([p[0] for p in good_data_test], \n",
    "                                                evaluator=llm_evaluator,\n",
    "                                                generator=llm_generator_with_picker,\n",
    "                                                rubric=rubric_good,\n",
    "                                                phi=phi,\n",
    "                                                max_rounds=rounds)\n",
    "\n",
    "other_labels, other_successes, other_flips = no_data_algorithm_with_flips([p[0] for p in other_data_test], \n",
    "                                                  evaluator=llm_evaluator,\n",
    "                                                  generator=llm_generator_with_picker,\n",
    "                                                  rubric=rubric_other,\n",
    "                                                  phi=phi,\n",
    "                                                  max_rounds=rounds,)\n",
    "\n",
    "print_metrics(good_labels, good_successes, good_data_test, \"In\", good_flips)\n",
    "print_metrics(other_labels, other_successes, other_data_test, \"Out\", other_flips)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2b8d01d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 498/498 [18:14:34<00:00, 131.88s/it]     \n",
      "100%|██████████| 498/498 [6:12:11<00:00, 44.84s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In-phenomenon test score: 0.976 | Successes: 81.325 | F1: 97.581 | Flips: 1.807\n",
      "Out-phenomenon test score: 0.59 | Successes: 27.912 | F1: 63.958 | Flips: 6.024\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "phi = 0.9 #o3-mini, picker\n",
    "MODEL = \"gpt-o3-mini\"\n",
    "params = {\"max_completion_tokens\": 50000} #, \"temperature\": 0.0} \n",
    "llm = LLMClient(params, MODEL)\n",
    "\n",
    "good_labels, good_successes, good_flips = no_data_algorithm_with_flips([p[0] for p in good_data_test], \n",
    "                                                evaluator=llm_evaluator,\n",
    "                                                generator=llm_generator_with_picker,\n",
    "                                                rubric=rubric_good,\n",
    "                                                phi=phi,\n",
    "                                                max_rounds=rounds)\n",
    "\n",
    "other_labels, other_successes, other_flips = no_data_algorithm_with_flips([p[0] for p in other_data_test], \n",
    "                                                  evaluator=llm_evaluator,\n",
    "                                                  generator=llm_generator_with_picker,\n",
    "                                                  rubric=rubric_other,\n",
    "                                                  phi=phi,\n",
    "                                                  max_rounds=rounds,)\n",
    "\n",
    "print_metrics(good_labels, good_successes, good_data_test, \"In\", good_flips)\n",
    "print_metrics(other_labels, other_successes, other_data_test, \"Out\", other_flips)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "45584fe4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 498/498 [10:18:07<00:00, 74.47s/it]   \n",
      "100%|██████████| 498/498 [5:47:07<00:00, 41.82s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In-phenomenon test score: 0.894 | Successes: 80.723 | F1: 88.795 | Flips: 10.442\n",
      "Out-phenomenon test score: 0.534 | Successes: 27.912 | F1: 55.725 | Flips: 37.349\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "phi = 0.5 #o3-mini, picker\n",
    "MODEL = \"gpt-o3-mini\"\n",
    "params = {\"max_completion_tokens\": 50000} #, \"temperature\": 0.0} \n",
    "llm = LLMClient(params, MODEL)\n",
    "\n",
    "good_labels, good_successes, good_flips = no_data_algorithm_with_flips([p[0] for p in good_data_test], \n",
    "                                                evaluator=llm_evaluator,\n",
    "                                                generator=llm_generator_with_picker,\n",
    "                                                rubric=rubric_good,\n",
    "                                                phi=phi,\n",
    "                                                max_rounds=rounds)\n",
    "\n",
    "other_labels, other_successes, other_flips = no_data_algorithm_with_flips([p[0] for p in other_data_test], \n",
    "                                                  evaluator=llm_evaluator,\n",
    "                                                  generator=llm_generator_with_picker,\n",
    "                                                  rubric=rubric_other,\n",
    "                                                  phi=phi,\n",
    "                                                  max_rounds=rounds,)\n",
    "\n",
    "print_metrics(good_labels, good_successes, good_data_test, \"In\", good_flips)\n",
    "print_metrics(other_labels, other_successes, other_data_test, \"Out\", other_flips)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "40221c81",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 498/498 [10:51:35<00:00, 78.51s/it]  \n",
      "100%|██████████| 498/498 [7:08:04<00:00, 51.58s/it]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In-phenomenon test score: 0.898 | Successes: 79.518 | F1: 89.441 | Flips: 9.639\n",
      "Out-phenomenon test score: 0.574 | Successes: 27.912 | F1: 60.0 | Flips: 30.723\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "phi = 0.6 #o3-mini, picker\n",
    "MODEL = \"gpt-o3-mini\"\n",
    "params = {\"max_completion_tokens\": 50000} #, \"temperature\": 0.0} \n",
    "llm = LLMClient(params, MODEL)\n",
    "\n",
    "good_labels, good_successes, good_flips = no_data_algorithm_with_flips([p[0] for p in good_data_test], \n",
    "                                                evaluator=llm_evaluator,\n",
    "                                                generator=llm_generator_with_picker,\n",
    "                                                rubric=rubric_good,\n",
    "                                                phi=phi,\n",
    "                                                max_rounds=rounds)\n",
    "\n",
    "other_labels, other_successes, other_flips = no_data_algorithm_with_flips([p[0] for p in other_data_test], \n",
    "                                                  evaluator=llm_evaluator,\n",
    "                                                  generator=llm_generator_with_picker,\n",
    "                                                  rubric=rubric_other,\n",
    "                                                  phi=phi,\n",
    "                                                  max_rounds=rounds,)\n",
    "\n",
    "print_metrics(good_labels, good_successes, good_data_test, \"In\", good_flips)\n",
    "print_metrics(other_labels, other_successes, other_data_test, \"Out\", other_flips)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ed5ef82",
   "metadata": {},
   "source": [
    "GPT-4o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d25a9efd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 498/498 [4:02:51<00:00, 29.26s/it]  \n",
      "100%|██████████| 498/498 [2:03:09<00:00, 14.84s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In-phenomenon test score: 0.371 | Successes: 28.715 | F1: 27.714 | Flips: 71.285\n",
      "Out-phenomenon test score: 0.404 | Successes: 9.438 | F1: 20.375 | Flips: 90.562\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "phi = 0.0 #gpt-4o, picker\n",
    "MODEL = \"gpt-4o-2024-05-13\"\n",
    "params = {\"max_tokens\": 1024, \"temperature\": 0.0} \n",
    "llm = LLMClient(params, MODEL)\n",
    "\n",
    "good_labels, good_successes, good_flips = no_data_algorithm_with_flips([p[0] for p in good_data_test], \n",
    "                                                evaluator=llm_evaluator,\n",
    "                                                generator=llm_generator_with_picker,\n",
    "                                                rubric=rubric_good,\n",
    "                                                phi=phi,\n",
    "                                                max_rounds=rounds)\n",
    "\n",
    "other_labels, other_successes, other_flips = no_data_algorithm_with_flips([p[0] for p in other_data_test], \n",
    "                                                  evaluator=llm_evaluator,\n",
    "                                                  generator=llm_generator_with_picker,\n",
    "                                                  rubric=rubric_other,\n",
    "                                                  phi=phi,\n",
    "                                                  max_rounds=rounds,)\n",
    "\n",
    "print_metrics(good_labels, good_successes, good_data_test, \"In\", good_flips)\n",
    "print_metrics(other_labels, other_successes, other_data_test, \"Out\", other_flips)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "21fb7d4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 498/498 [2:55:23<00:00, 21.13s/it]  \n",
      "100%|██████████| 498/498 [2:39:31<00:00, 19.22s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In-phenomenon test score: 0.58 | Successes: 28.715 | F1: 67.087 | Flips: 5.622\n",
      "Out-phenomenon test score: 0.564 | Successes: 10.843 | F1: 65.391 | Flips: 8.835\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "phi = 0.9 #gpt-4o, picker\n",
    "MODEL = \"gpt-4o-2024-05-13\"\n",
    "params = {\"max_tokens\": 1024, \"temperature\": 0.0} \n",
    "llm = LLMClient(params, MODEL)\n",
    "\n",
    "good_labels, good_successes, good_flips = no_data_algorithm_with_flips([p[0] for p in good_data_test], \n",
    "                                                evaluator=llm_evaluator,\n",
    "                                                generator=llm_generator_with_picker,\n",
    "                                                rubric=rubric_good,\n",
    "                                                phi=phi,\n",
    "                                                max_rounds=rounds)\n",
    "\n",
    "other_labels, other_successes, other_flips = no_data_algorithm_with_flips([p[0] for p in other_data_test], \n",
    "                                                  evaluator=llm_evaluator,\n",
    "                                                  generator=llm_generator_with_picker,\n",
    "                                                  rubric=rubric_other,\n",
    "                                                  phi=phi,\n",
    "                                                  max_rounds=rounds,)\n",
    "\n",
    "print_metrics(good_labels, good_successes, good_data_test, \"In\", good_flips)\n",
    "print_metrics(other_labels, other_successes, other_data_test, \"Out\", other_flips)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e453c456",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 498/498 [3:07:32<00:00, 22.60s/it]  \n",
      "100%|██████████| 498/498 [3:55:23<00:00, 28.36s/it]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In-phenomenon test score: 0.508 | Successes: 30.924 | F1: 54.545 | Flips: 33.534\n",
      "Out-phenomenon test score: 0.486 | Successes: 8.835 | F1: 49.606 | Flips: 46.988\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "phi = 0.5 #gpt-4o, picker\n",
    "MODEL = \"gpt-4o-2024-05-13\"\n",
    "params = {\"max_tokens\": 1024, \"temperature\": 0.0} \n",
    "llm = LLMClient(params, MODEL)\n",
    "\n",
    "good_labels, good_successes, good_flips = no_data_algorithm_with_flips([p[0] for p in good_data_test], \n",
    "                                                evaluator=llm_evaluator,\n",
    "                                                generator=llm_generator_with_picker,\n",
    "                                                rubric=rubric_good,\n",
    "                                                phi=phi,\n",
    "                                                max_rounds=rounds)\n",
    "\n",
    "other_labels, other_successes, other_flips = no_data_algorithm_with_flips([p[0] for p in other_data_test], \n",
    "                                                  evaluator=llm_evaluator,\n",
    "                                                  generator=llm_generator_with_picker,\n",
    "                                                  rubric=rubric_other,\n",
    "                                                  phi=phi,\n",
    "                                                  max_rounds=rounds,)\n",
    "\n",
    "print_metrics(good_labels, good_successes, good_data_test, \"In\", good_flips)\n",
    "print_metrics(other_labels, other_successes, other_data_test, \"Out\", other_flips)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8d32efa0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 498/498 [6:05:10<00:00, 44.00s/it]   \n",
      "100%|██████████| 498/498 [2:19:07<00:00, 16.76s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In-phenomenon test score: 0.482 | Successes: 25.301 | F1: 52.574 | Flips: 29.518\n",
      "Out-phenomenon test score: 0.514 | Successes: 10.643 | F1: 54.851 | Flips: 35.141\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "phi = 0.6 #gpt-4o, picker\n",
    "MODEL = \"gpt-4o-2024-05-13\"\n",
    "params = {\"max_tokens\": 1024, \"temperature\": 0.0} \n",
    "llm = LLMClient(params, MODEL)\n",
    "\n",
    "good_labels, good_successes, good_flips = no_data_algorithm_with_flips([p[0] for p in good_data_test], \n",
    "                                                evaluator=llm_evaluator,\n",
    "                                                generator=llm_generator_with_picker,\n",
    "                                                rubric=rubric_good,\n",
    "                                                phi=phi,\n",
    "                                                max_rounds=rounds)\n",
    "\n",
    "other_labels, other_successes, other_flips = no_data_algorithm_with_flips([p[0] for p in other_data_test], \n",
    "                                                  evaluator=llm_evaluator,\n",
    "                                                  generator=llm_generator_with_picker,\n",
    "                                                  rubric=rubric_other,\n",
    "                                                  phi=phi,\n",
    "                                                  max_rounds=rounds,)\n",
    "\n",
    "print_metrics(good_labels, good_successes, good_data_test, \"In\", good_flips)\n",
    "print_metrics(other_labels, other_successes, other_data_test, \"Out\", other_flips)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10d0215a",
   "metadata": {},
   "source": [
    "DeepSeek-R1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6cba8356",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 498/498 [42:10:54<00:00, 304.93s/it]   \n",
      "100%|██████████| 498/498 [31:19:05<00:00, 226.40s/it]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In-phenomenon test score: 0.42 | Successes: 24.498 | F1: 32.947 | Flips: 75.502\n",
      "Out-phenomenon test score: 0.396 | Successes: 11.647 | F1: 19.303 | Flips: 88.353\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "phi = 0.0 #deepseek, picker\n",
    "MODEL = \"deepseek-r1-distill-qwen-32b\"\n",
    "params = {\"max_tokens\": 2048, \"temperature\": 0.0} \n",
    "llm = LLMClient(params, MODEL)\n",
    "\n",
    "good_labels, good_successes, good_flips = no_data_algorithm_with_flips([p[0] for p in good_data_test], \n",
    "                                                evaluator=llm_evaluator,\n",
    "                                                generator=llm_generator_with_picker,\n",
    "                                                rubric=rubric_good,\n",
    "                                                phi=phi,\n",
    "                                                max_rounds=rounds)\n",
    "\n",
    "other_labels, other_successes, other_flips = no_data_algorithm_with_flips([p[0] for p in other_data_test], \n",
    "                                                  evaluator=llm_evaluator,\n",
    "                                                  generator=llm_generator_with_picker,\n",
    "                                                  rubric=rubric_other,\n",
    "                                                  phi=phi,\n",
    "                                                  max_rounds=rounds,)\n",
    "\n",
    "print_metrics(good_labels, good_successes, good_data_test, \"In\", good_flips)\n",
    "print_metrics(other_labels, other_successes, other_data_test, \"Out\", other_flips)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "481580bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 498/498 [44:03:13<00:00, 318.46s/it]     \n",
      "100%|██████████| 498/498 [36:01:40<00:00, 260.44s/it]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In-phenomenon test score: 0.598 | Successes: 26.506 | F1: 68.454 | Flips: 7.43\n",
      "Out-phenomenon test score: 0.526 | Successes: 6.627 | F1: 62.42 | Flips: 11.847\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "phi = 0.9 #deepseek, picker\n",
    "MODEL = \"deepseek-r1-distill-qwen-32b\"\n",
    "params = {\"max_tokens\": 2048, \"temperature\": 0.0} \n",
    "llm = LLMClient(params, MODEL)\n",
    "\n",
    "good_labels, good_successes, good_flips = no_data_algorithm_with_flips([p[0] for p in good_data_test], \n",
    "                                                evaluator=llm_evaluator,\n",
    "                                                generator=llm_generator_with_picker,\n",
    "                                                rubric=rubric_good,\n",
    "                                                phi=phi,\n",
    "                                                max_rounds=rounds)\n",
    "\n",
    "other_labels, other_successes, other_flips = no_data_algorithm_with_flips([p[0] for p in other_data_test], \n",
    "                                                  evaluator=llm_evaluator,\n",
    "                                                  generator=llm_generator_with_picker,\n",
    "                                                  rubric=rubric_other,\n",
    "                                                  phi=phi,\n",
    "                                                  max_rounds=rounds,)\n",
    "\n",
    "print_metrics(good_labels, good_successes, good_data_test, \"In\", good_flips)\n",
    "print_metrics(other_labels, other_successes, other_data_test, \"Out\", other_flips)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cd7fb7b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 498/498 [45:55:56<00:00, 332.04s/it]   \n",
      "100%|██████████| 498/498 [38:23:24<00:00, 277.52s/it]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In-phenomenon test score: 0.508 | Successes: 16.466 | F1: 55.21 | Flips: 38.554\n",
      "Out-phenomenon test score: 0.48 | Successes: 6.225 | F1: 49.116 | Flips: 50.402\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "phi = 0.5 #deepseek, picker\n",
    "MODEL = \"deepseek-r1-distill-qwen-32b\"\n",
    "params = {\"max_tokens\": 2048, \"temperature\": 0.0} \n",
    "llm = LLMClient(params, MODEL)\n",
    "\n",
    "\n",
    "good_labels, good_successes, good_flips = no_data_algorithm_with_flips([p[0] for p in good_data_test], \n",
    "                                                evaluator=llm_evaluator,\n",
    "                                                generator=llm_generator_with_picker,\n",
    "                                                rubric=rubric_good,\n",
    "                                                phi=phi,\n",
    "                                                max_rounds=rounds)\n",
    "\n",
    "other_labels, other_successes, other_flips = no_data_algorithm_with_flips([p[0] for p in other_data_test], \n",
    "                                                  evaluator=llm_evaluator,\n",
    "                                                  generator=llm_generator_with_picker,\n",
    "                                                  rubric=rubric_other,\n",
    "                                                  phi=phi,\n",
    "                                                  max_rounds=rounds,)\n",
    "\n",
    "print_metrics(good_labels, good_successes, good_data_test, \"In\", good_flips)\n",
    "print_metrics(other_labels, other_successes, other_data_test, \"Out\", other_flips)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3b9499f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 498/498 [44:06:29<00:00, 318.85s/it]   \n",
      "100%|██████████| 498/498 [40:08:51<00:00, 290.22s/it]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In-phenomenon test score: 0.52 | Successes: 15.261 | F1: 56.781 | Flips: 36.145\n",
      "Out-phenomenon test score: 0.48 | Successes: 5.622 | F1: 50.478 | Flips: 40.161\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "phi = 0.6 #deepseek, picker\n",
    "MODEL = \"deepseek-r1-distill-qwen-32b\"\n",
    "params = {\"max_tokens\": 2048, \"temperature\": 0.0} \n",
    "llm = LLMClient(params, MODEL)\n",
    "\n",
    "good_labels, good_successes, good_flips = no_data_algorithm_with_flips([p[0] for p in good_data_test], \n",
    "                                                evaluator=llm_evaluator,\n",
    "                                                generator=llm_generator_with_picker,\n",
    "                                                rubric=rubric_good,\n",
    "                                                phi=phi,\n",
    "                                                max_rounds=rounds)\n",
    "\n",
    "other_labels, other_successes, other_flips = no_data_algorithm_with_flips([p[0] for p in other_data_test], \n",
    "                                                  evaluator=llm_evaluator,\n",
    "                                                  generator=llm_generator_with_picker,\n",
    "                                                  rubric=rubric_other,\n",
    "                                                  phi=phi,\n",
    "                                                  max_rounds=rounds,)\n",
    "\n",
    "print_metrics(good_labels, good_successes, good_data_test, \"In\", good_flips)\n",
    "print_metrics(other_labels, other_successes, other_data_test, \"Out\", other_flips)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aec3693a",
   "metadata": {},
   "source": [
    "Qwen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3947d605",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 498/498 [1:07:53<00:00,  8.18s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In-phenomenon test score: 0.54 | Successes: 13.253 | F1: 27.302 | Flips: 86.747\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 498/498 [1:11:08<00:00,  8.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out-phenomenon test score: 0.335 | Successes: 16.265 | F1: 0.0 | Flips: 83.735\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "phi = 0.0 # qwen, with phi picked based on baseline perf\n",
    "MODEL = \"qwen-25-vl7b\"\n",
    "params = {\"max_tokens\": 1024, \"temperature\": 0.0} \n",
    "llm = LLMClient(params, MODEL)\n",
    "\n",
    "good_labels, good_successes, good_flips = no_data_algorithm_with_flips([p[0] for p in good_data_test], \n",
    "                                                evaluator=llm_evaluator,\n",
    "                                                generator=llm_generator,\n",
    "                                                rubric=rubric_good,\n",
    "                                                phi=phi,\n",
    "                                                max_rounds=rounds)\n",
    "print_metrics(good_labels, good_successes, good_data_test, \"In\", good_flips)\n",
    "\n",
    "other_labels, other_successes, other_flips = no_data_algorithm_with_flips([p[0] for p in other_data_test], \n",
    "                                                  evaluator=llm_evaluator,\n",
    "                                                  generator=llm_generator,\n",
    "                                                  rubric=rubric_other,\n",
    "                                                  phi=phi,\n",
    "                                                  max_rounds=rounds,)\n",
    "\n",
    "print_metrics(other_labels, other_successes, other_data_test, \"Out\", other_flips)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e02664fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 498/498 [1:07:03<00:00,  8.08s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In-phenomenon test score: 0.512 | Successes: 13.052 | F1: 65.434 | Flips: 8.835\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 498/498 [1:10:58<00:00,  8.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out-phenomenon test score: 0.502 | Successes: 16.265 | F1: 64.873 | Flips: 8.032\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "phi = 0.9 # qwen, with phi picked based on baseline perf\n",
    "MODEL = \"qwen-25-vl7b\"\n",
    "params = {\"max_tokens\": 1024, \"temperature\": 0.0} \n",
    "llm = LLMClient(params, MODEL)\n",
    "\n",
    "good_labels, good_successes, good_flips = no_data_algorithm_with_flips([p[0] for p in good_data_test], \n",
    "                                                evaluator=llm_evaluator,\n",
    "                                                generator=llm_generator,\n",
    "                                                rubric=rubric_good,\n",
    "                                                phi=phi,\n",
    "                                                max_rounds=rounds)\n",
    "print_metrics(good_labels, good_successes, good_data_test, \"In\", good_flips)\n",
    "\n",
    "other_labels, other_successes, other_flips = no_data_algorithm_with_flips([p[0] for p in other_data_test], \n",
    "                                                  evaluator=llm_evaluator,\n",
    "                                                  generator=llm_generator,\n",
    "                                                  rubric=rubric_other,\n",
    "                                                  phi=phi,\n",
    "                                                  max_rounds=rounds,)\n",
    "\n",
    "print_metrics(other_labels, other_successes, other_data_test, \"Out\", other_flips)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bf0db41c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 498/498 [1:07:49<00:00,  8.17s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In-phenomenon test score: 0.532 | Successes: 13.052 | F1: 54.224 | Flips: 47.791\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 498/498 [1:10:38<00:00,  8.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out-phenomenon test score: 0.444 | Successes: 16.265 | F1: 47.834 | Flips: 43.574\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "phi = 0.5 # qwen, with phi picked based on baseline perf\n",
    "MODEL = \"qwen-25-vl7b\"\n",
    "params = {\"max_tokens\": 1024, \"temperature\": 0.0} \n",
    "llm = LLMClient(params, MODEL)\n",
    "\n",
    "good_labels, good_successes, good_flips = no_data_algorithm_with_flips([p[0] for p in good_data_test], \n",
    "                                                evaluator=llm_evaluator,\n",
    "                                                generator=llm_generator,\n",
    "                                                rubric=rubric_good,\n",
    "                                                phi=phi,\n",
    "                                                max_rounds=rounds)\n",
    "print_metrics(good_labels, good_successes, good_data_test, \"In\", good_flips)\n",
    "\n",
    "other_labels, other_successes, other_flips = no_data_algorithm_with_flips([p[0] for p in other_data_test], \n",
    "                                                  evaluator=llm_evaluator,\n",
    "                                                  generator=llm_generator,\n",
    "                                                  rubric=rubric_other,\n",
    "                                                  phi=phi,\n",
    "                                                  max_rounds=rounds,)\n",
    "\n",
    "print_metrics(other_labels, other_successes, other_data_test, \"Out\", other_flips)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "478ffcf8",
   "metadata": {},
   "source": [
    "## Ablation -- Generator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fb2361c",
   "metadata": {},
   "source": [
    "### ND Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1943a7fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 498/498 [15:03:31<00:00, 108.86s/it]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In-phenomenon test score: 0.54 | Successes: 53.815 | F1: 58.887 | Flips: 46.185\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 498/498 [9:29:31<00:00, 68.62s/it]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out-phenomenon test score: 0.584 | Successes: 26.305 | F1: 56.604 | Flips: 73.695\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "phi = 0.0 # o3-mini, with phi picked based on baseline perf\n",
    "\n",
    "MODEL = \"gpt-o3-mini\"\n",
    "params = {\"max_completion_tokens\": 50000} #, \"temperature\": 0.0} \n",
    "llm = LLMClient(params, MODEL)\n",
    "\n",
    "good_labels, good_successes, good_flips = no_data_algorithm_with_flips([p[0] for p in good_data_test], \n",
    "                                                evaluator=llm_evaluator,\n",
    "                                                generator=llm_generator,\n",
    "                                                rubric=rubric_good,\n",
    "                                                phi=phi,\n",
    "                                                max_rounds=rounds)\n",
    "print_metrics(good_labels, good_successes, good_data_test, \"In\", good_flips)\n",
    "\n",
    "other_labels, other_successes, other_flips = no_data_algorithm_with_flips([p[0] for p in other_data_test], \n",
    "                                                  evaluator=llm_evaluator,\n",
    "                                                  generator=llm_generator,\n",
    "                                                  rubric=rubric_other,\n",
    "                                                  phi=phi,\n",
    "                                                  max_rounds=rounds,)\n",
    "\n",
    "print_metrics(other_labels, other_successes, other_data_test, \"Out\", other_flips)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a56e7f62",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 498/498 [14:35:06<00:00, 105.43s/it]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In-phenomenon test score: 0.942 | Successes: 55.422 | F1: 94.235 | Flips: 5.02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 498/498 [10:38:16<00:00, 76.90s/it]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out-phenomenon test score: 0.606 | Successes: 28.715 | F1: 65.493 | Flips: 7.831\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "phi = 0.9 # o3-mini, with phi picked based on baseline perf\n",
    "\n",
    "MODEL = \"gpt-o3-mini\"\n",
    "params = {\"max_completion_tokens\": 50000} #, \"temperature\": 0.0} \n",
    "llm = LLMClient(params, MODEL)\n",
    "\n",
    "good_labels, good_successes, good_flips = no_data_algorithm_with_flips([p[0] for p in good_data_test], \n",
    "                                                evaluator=llm_evaluator,\n",
    "                                                generator=llm_generator,\n",
    "                                                rubric=rubric_good,\n",
    "                                                phi=phi,\n",
    "                                                max_rounds=rounds)\n",
    "print_metrics(good_labels, good_successes, good_data_test, \"In\", good_flips)\n",
    "\n",
    "other_labels, other_successes, other_flips = no_data_algorithm_with_flips([p[0] for p in other_data_test], \n",
    "                                                  evaluator=llm_evaluator,\n",
    "                                                  generator=llm_generator,\n",
    "                                                  rubric=rubric_other,\n",
    "                                                  phi=phi,\n",
    "                                                  max_rounds=rounds,)\n",
    "\n",
    "print_metrics(other_labels, other_successes, other_data_test, \"Out\", other_flips)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5803833c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 498/498 [13:43:47<00:00, 99.25s/it]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In-phenomenon test score: 0.815 | Successes: 58.233 | F1: 82.375 | Flips: 18.474\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 498/498 [9:32:06<00:00, 68.93s/it]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out-phenomenon test score: 0.564 | Successes: 28.715 | F1: 59.287 | Flips: 35.944\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "phi = 0.5 # o3-mini, with phi picked based on baseline perf\n",
    "\n",
    "MODEL = \"gpt-o3-mini\"\n",
    "params = {\"max_completion_tokens\": 50000} #, \"temperature\": 0.0} \n",
    "llm = LLMClient(params, MODEL)\n",
    "\n",
    "good_labels, good_successes, good_flips = no_data_algorithm_with_flips([p[0] for p in good_data_test], \n",
    "                                                evaluator=llm_evaluator,\n",
    "                                                generator=llm_generator,\n",
    "                                                rubric=rubric_good,\n",
    "                                                phi=phi,\n",
    "                                                max_rounds=rounds)\n",
    "print_metrics(good_labels, good_successes, good_data_test, \"In\", good_flips)\n",
    "\n",
    "other_labels, other_successes, other_flips = no_data_algorithm_with_flips([p[0] for p in other_data_test], \n",
    "                                                  evaluator=llm_evaluator,\n",
    "                                                  generator=llm_generator,\n",
    "                                                  rubric=rubric_other,\n",
    "                                                  phi=phi,\n",
    "                                                  max_rounds=rounds,)\n",
    "\n",
    "print_metrics(other_labels, other_successes, other_data_test, \"Out\", other_flips)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e960a31",
   "metadata": {},
   "source": [
    "GPT-4o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8ab67dbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 498/498 [1:48:04<00:00, 13.02s/it]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In-phenomenon test score: 0.428 | Successes: 6.627 | F1: 23.592 | Flips: 93.373\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 498/498 [4:27:02<00:00, 32.17s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out-phenomenon test score: 0.787 | Successes: 34.137 | F1: 78.099 | Flips: 65.863\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "phi = 0.0 # gpt-4o, with phi picked based on baseline perf\n",
    "\n",
    "MODEL = \"gpt-4o-2024-05-13\"\n",
    "params = {\"max_tokens\": 1024, \"temperature\": 0.0} \n",
    "llm = LLMClient(params, MODEL)\n",
    "\n",
    "good_labels, good_successes, good_flips = no_data_algorithm_with_flips([p[0] for p in good_data_test], \n",
    "                                                evaluator=llm_evaluator,\n",
    "                                                generator=llm_generator,\n",
    "                                                rubric=rubric_good,\n",
    "                                                phi=phi,\n",
    "                                                max_rounds=rounds)\n",
    "print_metrics(good_labels, good_successes, good_data_test, \"In\", good_flips)\n",
    "\n",
    "other_labels, other_successes, other_flips = no_data_algorithm_with_flips([p[0] for p in other_data_test], \n",
    "                                                  evaluator=llm_evaluator,\n",
    "                                                  generator=llm_generator,\n",
    "                                                  rubric=rubric_other,\n",
    "                                                  phi=phi,\n",
    "                                                  max_rounds=rounds,)\n",
    "\n",
    "print_metrics(other_labels, other_successes, other_data_test, \"Out\", other_flips)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e0a66543",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 498/498 [2:52:16<00:00, 20.76s/it]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In-phenomenon test score: 0.566 | Successes: 5.823 | F1: 63.758 | Flips: 10.843\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 498/498 [2:25:13<00:00, 17.50s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out-phenomenon test score: 0.534 | Successes: 34.538 | F1: 63.636 | Flips: 4.618\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "phi = 0.9 # gpt-4o, with phi picked based on baseline perf\n",
    "\n",
    "MODEL = \"gpt-4o-2024-05-13\"\n",
    "params = {\"max_tokens\": 1024, \"temperature\": 0.0} \n",
    "llm = LLMClient(params, MODEL)\n",
    "\n",
    "good_labels, good_successes, good_flips = no_data_algorithm_with_flips([p[0] for p in good_data_test], \n",
    "                                                evaluator=llm_evaluator,\n",
    "                                                generator=llm_generator,\n",
    "                                                rubric=rubric_good,\n",
    "                                                phi=phi,\n",
    "                                                max_rounds=rounds)\n",
    "print_metrics(good_labels, good_successes, good_data_test, \"In\", good_flips)\n",
    "\n",
    "other_labels, other_successes, other_flips = no_data_algorithm_with_flips([p[0] for p in other_data_test], \n",
    "                                                  evaluator=llm_evaluator,\n",
    "                                                  generator=llm_generator,\n",
    "                                                  rubric=rubric_other,\n",
    "                                                  phi=phi,\n",
    "                                                  max_rounds=rounds,)\n",
    "\n",
    "print_metrics(other_labels, other_successes, other_data_test, \"Out\", other_flips)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9454dc9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 498/498 [1:10:26<00:00,  8.49s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In-phenomenon test score: 0.486 | Successes: 6.627 | F1: 48.8 | Flips: 47.791\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 498/498 [1:33:05<00:00, 11.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out-phenomenon test score: 0.633 | Successes: 33.735 | F1: 67.38 | Flips: 34.137\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "phi = 0.5 # gpt-4o, with phi picked based on baseline perf\n",
    "\n",
    "MODEL = \"gpt-4o-2024-05-13\"\n",
    "params = {\"max_tokens\": 1024, \"temperature\": 0.0} \n",
    "llm = LLMClient(params, MODEL)\n",
    "\n",
    "good_labels, good_successes, good_flips = no_data_algorithm_with_flips([p[0] for p in good_data_test], \n",
    "                                                evaluator=llm_evaluator,\n",
    "                                                generator=llm_generator,\n",
    "                                                rubric=rubric_good,\n",
    "                                                phi=phi,\n",
    "                                                max_rounds=rounds)\n",
    "print_metrics(good_labels, good_successes, good_data_test, \"In\", good_flips)\n",
    "\n",
    "other_labels, other_successes, other_flips = no_data_algorithm_with_flips([p[0] for p in other_data_test], \n",
    "                                                  evaluator=llm_evaluator,\n",
    "                                                  generator=llm_generator,\n",
    "                                                  rubric=rubric_other,\n",
    "                                                  phi=phi,\n",
    "                                                  max_rounds=rounds,)\n",
    "\n",
    "print_metrics(other_labels, other_successes, other_data_test, \"Out\", other_flips)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
